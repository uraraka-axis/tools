"""
R-Cabinet ç®¡ç†ãƒ„ãƒ¼ãƒ«
- ãƒ•ã‚©ãƒ«ãƒ€ç”»åƒä¸€è¦§ï¼šR-Cabinetã®ãƒ•ã‚©ãƒ«ãƒ€æ¯ã«ç”»åƒã‚’ä¸€è¦§è¡¨ç¤º
- ç”»åƒå­˜åœ¨ãƒã‚§ãƒƒã‚¯ï¼šã‚³ãƒŸãƒƒã‚¯Noã‚’å…¥åŠ›ã—ã¦å­˜åœ¨ç¢ºèª
"""

# ãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼ˆãƒ‡ãƒ—ãƒ­ã‚¤ç¢ºèªç”¨ï¼‰
APP_VERSION = "2.1.0"

import streamlit as st
import requests
import base64
import xml.etree.ElementTree as ET
import pandas as pd
import time
from io import BytesIO
from datetime import datetime

# é‡ã„ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯é…å»¶èª­ã¿è¾¼ã¿ï¼ˆèµ·å‹•é«˜é€ŸåŒ–ï¼‰
_bs4_module = None
_openpyxl_styles = None
_openpyxl_utils = None
_supabase_module = None
_zipfile_module = None
_random_module = None

# Gemini AIï¼ˆé…å»¶èª­ã¿è¾¼ã¿ - èµ·å‹•é«˜é€ŸåŒ–ã®ãŸã‚ï¼‰
GEMINI_AVAILABLE = None
_genai_module = None


def get_bs4():
    """BeautifulSoupã‚’é…å»¶èª­ã¿è¾¼ã¿"""
    global _bs4_module
    if _bs4_module is None:
        from bs4 import BeautifulSoup
        _bs4_module = BeautifulSoup
    return _bs4_module


def get_openpyxl_styles():
    """openpyxlã‚¹ã‚¿ã‚¤ãƒ«ã‚’é…å»¶èª­ã¿è¾¼ã¿"""
    global _openpyxl_styles, _openpyxl_utils
    if _openpyxl_styles is None:
        from openpyxl.styles import Font, Border, Side, PatternFill, Alignment
        from openpyxl.utils import get_column_letter
        _openpyxl_styles = {'Font': Font, 'Border': Border, 'Side': Side, 'PatternFill': PatternFill, 'Alignment': Alignment}
        _openpyxl_utils = {'get_column_letter': get_column_letter}
    return _openpyxl_styles, _openpyxl_utils


def get_supabase_module():
    """Supabaseã‚’é…å»¶èª­ã¿è¾¼ã¿"""
    global _supabase_module
    if _supabase_module is None:
        from supabase import create_client
        _supabase_module = create_client
    return _supabase_module


def get_zipfile():
    """zipfileã‚’é…å»¶èª­ã¿è¾¼ã¿"""
    global _zipfile_module
    if _zipfile_module is None:
        import zipfile
        _zipfile_module = zipfile
    return _zipfile_module


def get_random():
    """randomã‚’é…å»¶èª­ã¿è¾¼ã¿"""
    global _random_module
    if _random_module is None:
        import random
        _random_module = random
    return _random_module

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="R-Cabinet ç®¡ç†ãƒ„ãƒ¼ãƒ«",
    page_icon="ğŸ–¼ï¸",
    layout="wide"
)

# èªè¨¼æƒ…å ±ï¼ˆStreamlit Secretsã‹ã‚‰å–å¾—ï¼‰
APP_PASSWORD = st.secrets.get("password", "")
SERVICE_SECRET = st.secrets.get("RMS_SERVICE_SECRET", "")
LICENSE_KEY = st.secrets.get("RMS_LICENSE_KEY", "")
BASE_URL = "https://api.rms.rakuten.co.jp/es/1.0"

# Supabaseæ¥ç¶šæƒ…å ±
SUPABASE_URL = st.secrets.get("SUPABASE_URL", "")
SUPABASE_KEY = st.secrets.get("SUPABASE_KEY", "")

# GitHubæ¥ç¶šæƒ…å ±
GITHUB_TOKEN = st.secrets.get("GITHUB_TOKEN", "")
GITHUB_REPO = "uraraka-axis/tools"
GITHUB_MISSING_CSV_PATH = "comic-lister/data/missing_comics.csv"
GITHUB_IS_LIST_PATH = "comic-lister/data/is_list.csv"
GITHUB_COMIC_LIST_PATH = "comic-lister/data/comic_list.csv"
GITHUB_FOLDER_HIERARCHY_PATH = "comic-lister/data/folder_hierarchy.xlsx"

# Gemini APIè¨­å®šï¼ˆã‚»ãƒ«ãƒ•ãƒ’ãƒ¼ãƒªãƒ³ã‚°ç”¨ï¼‰
GEMINI_API_KEY = st.secrets.get("GEMINI_API_KEY", "")


def get_gemini_model():
    """Gemini AIãƒ¢ãƒ‡ãƒ«ã‚’é…å»¶èª­ã¿è¾¼ã¿ã§å–å¾—"""
    global GEMINI_AVAILABLE, _genai_module

    if GEMINI_AVAILABLE is None:
        try:
            import google.generativeai as genai
            _genai_module = genai
            GEMINI_AVAILABLE = True
        except ImportError:
            GEMINI_AVAILABLE = False
            return None

    if not GEMINI_AVAILABLE or not GEMINI_API_KEY:
        return None

    if _genai_module:
        _genai_module.configure(api_key=GEMINI_API_KEY)
        return _genai_module.GenerativeModel('gemini-2.0-flash')

    return None


def upload_to_github(content: str, path: str, message: str) -> dict:
    """GitHubã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆä¸Šæ›¸ãæ›´æ–°ï¼‰"""
    if not GITHUB_TOKEN:
        return {"success": False, "error": "GITHUB_TOKENæœªè¨­å®š"}

    headers = {
        "Authorization": f"token {GITHUB_TOKEN}",
        "Accept": "application/vnd.github.v3+json"
    }

    # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã®SHAã‚’å–å¾—ï¼ˆæ›´æ–°æ™‚ã«å¿…è¦ï¼‰
    url = f"https://api.github.com/repos/{GITHUB_REPO}/contents/{path}"
    sha = None

    try:
        response = requests.get(url, headers=headers)
        if response.status_code == 200:
            sha = response.json().get("sha")
    except:
        pass

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    data = {
        "message": message,
        "content": base64.b64encode(content.encode('utf-8')).decode('utf-8'),
        "branch": "master"
    }
    if sha:
        data["sha"] = sha

    try:
        response = requests.put(url, headers=headers, json=data)
        if response.status_code in [200, 201]:
            return {"success": True, "url": response.json().get("content", {}).get("html_url", "")}
        else:
            return {"success": False, "error": f"HTTP {response.status_code}: {response.text[:200]}"}
    except Exception as e:
        return {"success": False, "error": str(e)}


def upload_binary_to_github(content: bytes, path: str, message: str) -> dict:
    """ãƒã‚¤ãƒŠãƒªãƒ•ã‚¡ã‚¤ãƒ«ã‚’GitHubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆä¸Šæ›¸ãæ›´æ–°ï¼‰"""
    if not GITHUB_TOKEN:
        return {"success": False, "error": "GITHUB_TOKENæœªè¨­å®š"}

    headers = {
        "Authorization": f"token {GITHUB_TOKEN}",
        "Accept": "application/vnd.github.v3+json"
    }

    # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã®SHAã‚’å–å¾—ï¼ˆæ›´æ–°æ™‚ã«å¿…è¦ï¼‰
    url = f"https://api.github.com/repos/{GITHUB_REPO}/contents/{path}"
    sha = None

    try:
        response = requests.get(url, headers=headers)
        if response.status_code == 200:
            sha = response.json().get("sha")
    except:
        pass

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    data = {
        "message": message,
        "content": base64.b64encode(content).decode('utf-8'),
        "branch": "master"
    }
    if sha:
        data["sha"] = sha

    try:
        response = requests.put(url, headers=headers, json=data)
        if response.status_code in [200, 201]:
            return {"success": True, "url": response.json().get("content", {}).get("html_url", "")}
        else:
            return {"success": False, "error": f"HTTP {response.status_code}: {response.text[:200]}"}
    except Exception as e:
        return {"success": False, "error": str(e)}


def download_from_github(path: str) -> dict:
    """GitHubã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    if not GITHUB_TOKEN:
        return {"success": False, "error": "GITHUB_TOKENæœªè¨­å®š"}

    headers = {
        "Authorization": f"token {GITHUB_TOKEN}",
        "Accept": "application/vnd.github.v3.raw"
    }

    url = f"https://api.github.com/repos/{GITHUB_REPO}/contents/{path}"

    try:
        response = requests.get(url, headers=headers)
        if response.status_code == 200:
            return {"success": True, "content": response.content, "path": path}
        elif response.status_code == 404:
            return {"success": False, "error": f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {path}"}
        else:
            return {"success": False, "error": f"HTTP {response.status_code}"}
    except Exception as e:
        return {"success": False, "error": str(e)}


def get_github_file_info(path: str) -> dict:
    """GitHubãƒ•ã‚¡ã‚¤ãƒ«ã®æƒ…å ±ï¼ˆæ›´æ–°æ—¥æ™‚ãªã©ï¼‰ã‚’å–å¾—"""
    if not GITHUB_TOKEN:
        return {}

    headers = {
        "Authorization": f"token {GITHUB_TOKEN}",
        "Accept": "application/vnd.github.v3+json"
    }

    url = f"https://api.github.com/repos/{GITHUB_REPO}/commits?path={path}&per_page=1"

    try:
        response = requests.get(url, headers=headers)
        if response.status_code == 200 and response.json():
            commit = response.json()[0]
            date_str = commit.get("commit", {}).get("committer", {}).get("date", "")
            if date_str:
                # ISOå½¢å¼ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦æ—¥æœ¬æ™‚é–“ã«å¤‰æ›ï¼ˆ+9æ™‚é–“ï¼‰
                from datetime import datetime, timedelta, timezone
                dt_utc = datetime.fromisoformat(date_str.replace("Z", "+00:00"))
                dt_jst = dt_utc + timedelta(hours=9)
                return {"last_updated": dt_jst.strftime("%Y-%m-%d %H:%M"), "exists": True}
        return {"exists": False}
    except:
        return {"exists": False}


def trigger_github_actions(workflow_file: str) -> dict:
    """GitHub Actionsãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’æ‰‹å‹•å®Ÿè¡Œ"""
    if not GITHUB_TOKEN:
        return {"success": False, "error": "GITHUB_TOKENæœªè¨­å®š"}

    headers = {
        "Authorization": f"token {GITHUB_TOKEN}",
        "Accept": "application/vnd.github.v3+json"
    }

    url = f"https://api.github.com/repos/{GITHUB_REPO}/actions/workflows/{workflow_file}/dispatches"

    try:
        response = requests.post(url, headers=headers, json={"ref": "master"})
        if response.status_code == 204:
            return {"success": True, "message": "ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’é–‹å§‹ã—ã¾ã—ãŸ"}
        elif response.status_code == 404:
            return {"success": False, "error": "ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"}
        else:
            return {"success": False, "error": f"HTTP {response.status_code}: {response.text[:200]}"}
    except Exception as e:
        return {"success": False, "error": str(e)}


def get_workflow_runs(workflow_file: str, limit: int = 3) -> list:
    """GitHub Actionsãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®å®Ÿè¡Œå±¥æ­´ã‚’å–å¾—"""
    if not GITHUB_TOKEN:
        return []

    headers = {
        "Authorization": f"token {GITHUB_TOKEN}",
        "Accept": "application/vnd.github.v3+json"
    }

    url = f"https://api.github.com/repos/{GITHUB_REPO}/actions/workflows/{workflow_file}/runs?per_page={limit}"

    try:
        response = requests.get(url, headers=headers)
        if response.status_code == 200:
            runs = response.json().get("workflow_runs", [])
            result = []
            for run in runs:
                created = run.get("created_at", "")
                if created:
                    dt = datetime.fromisoformat(created.replace("Z", "+00:00"))
                    created = dt.strftime("%Y-%m-%d %H:%M")
                result.append({
                    "status": run.get("status"),
                    "conclusion": run.get("conclusion"),
                    "created_at": created,
                    "html_url": run.get("html_url")
                })
            return result
        return []
    except:
        return []


@st.cache_resource
def get_supabase_client():
    """Supabaseã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å–å¾—ï¼ˆé…å»¶èª­ã¿è¾¼ã¿ï¼‰"""
    if SUPABASE_URL and SUPABASE_KEY:
        create_client = get_supabase_module()
        return create_client(SUPABASE_URL, SUPABASE_KEY)
    return None


def fetch_all_from_supabase(supabase: Client, table: str, columns: str = "*", filter_col: str = None, filter_val: str = None) -> list:
    """Supabaseã‹ã‚‰å…¨ä»¶å–å¾—ï¼ˆãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³å¯¾å¿œï¼‰"""
    all_data = []
    page_size = 1000
    offset = 0

    while True:
        query = supabase.table(table).select(columns).range(offset, offset + page_size - 1)
        if filter_col and filter_val:
            query = query.ilike(filter_col, f"%{filter_val}%")
        response = query.execute()

        if not response.data:
            break

        all_data.extend(response.data)

        if len(response.data) < page_size:
            break

        offset += page_size

    return all_data


def sync_images_to_db(images: list) -> dict:
    """ç”»åƒä¸€è¦§ã‚’DBã«åŒæœŸï¼ˆupsertï¼‰"""
    supabase = get_supabase_client()
    if not supabase:
        return {"success": False, "error": "Supabaseæœªè¨­å®š"}

    try:
        # file_nameã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼ˆé‡è¤‡æ¤œå‡ºï¼‰
        file_dict = {}
        for img in images:
            file_name = img.get("FileName", "")
            folder_name = img.get("FolderName", "")
            if file_name in file_dict:
                # é‡è¤‡: folder_namesã«è¿½åŠ 
                existing_folders = file_dict[file_name]["folder_names"].split(", ")
                if folder_name not in existing_folders:
                    file_dict[file_name]["folder_names"] += f", {folder_name}"
            else:
                file_dict[file_name] = {
                    "file_name": file_name,
                    "folder_names": folder_name,
                    "file_url": img.get("FileUrl", ""),
                    "file_size": img.get("FileSize", 0),
                    "file_timestamp": img.get("TimeStamp", "")
                }

        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³å¯¾å¿œï¼‰
        existing_data = fetch_all_from_supabase(supabase, "rcabinet_images", "file_name, file_timestamp")
        existing_dict = {row["file_name"]: row["file_timestamp"] for row in existing_data}

        # å·®åˆ†è¨ˆç®—
        new_count = 0
        updated_count = 0
        duplicate_count = 0
        unchanged_count = 0

        records_to_upsert = []
        for file_name, record in file_dict.items():
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆè¤‡æ•°ãƒ•ã‚©ãƒ«ãƒ€ã«ã‚ã‚‹ï¼‰
            if ", " in record["folder_names"]:
                duplicate_count += 1

            if file_name not in existing_dict:
                new_count += 1
                records_to_upsert.append(record)
            elif existing_dict[file_name] != record["file_timestamp"]:
                updated_count += 1
                records_to_upsert.append(record)
            else:
                unchanged_count += 1

        # å‰Šé™¤æ¸ˆã¿æ¤œå‡ºï¼ˆDBã«ã‚ã‚‹ãŒAPIã«ãªã„ï¼‰
        deleted_files = set(existing_dict.keys()) - set(file_dict.keys())
        deleted_count = len(deleted_files)

        # upsertå®Ÿè¡Œï¼ˆ100ä»¶ãšã¤ï¼‰
        for i in range(0, len(records_to_upsert), 100):
            batch = records_to_upsert[i:i+100]
            supabase.table("rcabinet_images").upsert(
                batch, on_conflict="file_name"
            ).execute()

        # å‰Šé™¤æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’DBã‹ã‚‰å‰Šé™¤
        if deleted_files:
            for file_name in deleted_files:
                supabase.table("rcabinet_images").delete().eq("file_name", file_name).execute()

        return {
            "success": True,
            "new": new_count,
            "updated": updated_count,
            "duplicate": duplicate_count,
            "unchanged": unchanged_count,
            "deleted": deleted_count,
            "total": len(file_dict)
        }
    except Exception as e:
        return {"success": False, "error": str(e)}


def load_images_from_db() -> tuple[list, str]:
    """DBã‹ã‚‰ç”»åƒä¸€è¦§ã‚’èª­ã¿è¾¼ã¿ï¼ˆãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³å¯¾å¿œï¼‰"""
    supabase = get_supabase_client()
    if not supabase:
        return [], "Supabaseæœªè¨­å®š"

    try:
        all_data = fetch_all_from_supabase(supabase, "rcabinet_images", "*")
        images = []
        for row in all_data:
            images.append({
                "FolderName": row.get("folder_names", ""),
                "FileName": row.get("file_name", ""),
                "FileUrl": row.get("file_url", ""),
                "FileSize": row.get("file_size", 0),
                "TimeStamp": row.get("file_timestamp", "")
            })
        return images, f"{len(images)}ä»¶ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ"
    except Exception as e:
        return [], str(e)


def get_db_stats() -> dict:
    """DBã®çµ±è¨ˆæƒ…å ±ã‚’å–å¾—ï¼ˆãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³å¯¾å¿œï¼‰"""
    supabase = get_supabase_client()
    if not supabase:
        return {}

    try:
        all_data = fetch_all_from_supabase(supabase, "rcabinet_images", "folder_names, created_at")
        total = len(all_data)
        duplicates = sum(1 for row in all_data if ", " in row.get("folder_names", ""))

        # æœ€çµ‚æ›´æ–°æ—¥æ™‚ã‚’å–å¾—
        last_updated = None
        if all_data:
            dates = [row.get("created_at") for row in all_data if row.get("created_at")]
            if dates:
                last_updated = max(dates)[:16].replace("T", " ")  # "2025-02-04 10:30"å½¢å¼

        return {"total": total, "duplicates": duplicates, "last_updated": last_updated}
    except Exception:
        return {}


def load_images_from_db_by_folder(folder_name: str) -> list:
    """DBã‹ã‚‰ç‰¹å®šãƒ•ã‚©ãƒ«ãƒ€ã®ç”»åƒã‚’èª­ã¿è¾¼ã¿ï¼ˆãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³å¯¾å¿œï¼‰"""
    supabase = get_supabase_client()
    if not supabase:
        return []

    try:
        all_data = fetch_all_from_supabase(supabase, "rcabinet_images", "*", "folder_names", folder_name)
        images = []
        for row in all_data:
            images.append({
                "FolderName": row.get("folder_names", ""),
                "FileName": row.get("file_name", ""),
                "FileUrl": row.get("file_url", ""),
                "FileSize": row.get("file_size", 0),
                "TimeStamp": row.get("file_timestamp", "")
            })
        return images
    except Exception:
        return []


def check_password():
    """ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰èªè¨¼"""
    if "authenticated" not in st.session_state:
        st.session_state.authenticated = False

    if st.session_state.authenticated:
        return True

    password_input = st.text_input("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", type="password")

    if password_input:
        if password_input == APP_PASSWORD:
            st.session_state.authenticated = True
            st.rerun()
        else:
            st.error("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“")

    return False


# ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰èªè¨¼
if not check_password():
    st.stop()


def get_auth_header():
    """ESAèªè¨¼ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ç”Ÿæˆ"""
    credentials = f"{SERVICE_SECRET}:{LICENSE_KEY}"
    encoded = base64.b64encode(credentials.encode()).decode()
    return {"Authorization": f"ESA {encoded}"}


def safe_int(value, default=0):
    """å®‰å…¨ã«intã«å¤‰æ›"""
    try:
        return int(value) if value else default
    except (ValueError, TypeError):
        return default


def style_excel(ws, num_columns=4, url_column=None):
    """Excelãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆã«ã‚¹ã‚¿ã‚¤ãƒ«ã‚’é©ç”¨"""
    styles, utils = get_openpyxl_styles()
    Font = styles['Font']
    Border = styles['Border']
    Side = styles['Side']
    PatternFill = styles['PatternFill']
    Alignment = styles['Alignment']
    get_column_letter = utils['get_column_letter']

    # ãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
    meiryo_font = Font(name='Meiryo UI')
    header_font = Font(name='Meiryo UI', bold=True, color='FFFFFF')
    # ç½«ç·šè¨­å®š
    thin_border = Border(
        left=Side(style='thin'),
        right=Side(style='thin'),
        top=Side(style='thin'),
        bottom=Side(style='thin')
    )
    # ãƒ˜ãƒƒãƒ€ãƒ¼èƒŒæ™¯è‰²ï¼ˆæ¿ƒã„é’ï¼‰
    header_fill = PatternFill(start_color='4472C4', end_color='4472C4', fill_type='solid')

    # å…¨ã‚»ãƒ«ã«ãƒ•ã‚©ãƒ³ãƒˆã¨ç½«ç·šã‚’é©ç”¨
    for row in ws.iter_rows(min_row=1, max_row=ws.max_row, min_col=1, max_col=num_columns):
        for cell in row:
            cell.font = meiryo_font
            cell.border = thin_border

    # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã®ã‚¹ã‚¿ã‚¤ãƒ«ï¼ˆ1è¡Œç›®ï¼‰
    for cell in ws[1]:
        if cell.column <= num_columns:
            cell.font = header_font
            cell.fill = header_fill
            cell.alignment = Alignment(horizontal='center', vertical='center')

    # åˆ—å¹…ã‚’è‡ªå‹•èª¿æ•´
    for col_idx in range(1, num_columns + 1):
        max_length = 0
        column_letter = get_column_letter(col_idx)
        for row in ws.iter_rows(min_row=1, max_row=ws.max_row, min_col=col_idx, max_col=col_idx):
            for cell in row:
                if cell.value:
                    cell_length = len(str(cell.value))
                    if cell_length > max_length:
                        max_length = cell_length
        # URLåˆ—ã¯å›ºå®šå¹…ã€ãã‚Œä»¥å¤–ã¯è‡ªå‹•èª¿æ•´
        if url_column and col_idx == url_column:
            ws.column_dimensions[column_letter].width = 70
        else:
            ws.column_dimensions[column_letter].width = min(max_length * 1.5 + 2, 40)


def merge_csv_data(is_df, cl_df):
    """ISæ¤œç´¢ã¨CLæ¤œç´¢ã®çµæœã‚’ãƒãƒ¼ã‚¸"""
    # comic_list.csvã‹ã‚‰è¾æ›¸ã‚’ä½œæˆï¼ˆNåˆ—=CNO, Såˆ—=å‡ºç‰ˆç¤¾, Yåˆ—=ã‚·ãƒªãƒ¼ã‚ºï¼‰
    cl_dict = {}
    for i in range(1, len(cl_df)):
        try:
            cno = str(cl_df.iloc[i, 13]).strip() if len(cl_df.columns) > 13 else ''  # Nåˆ—
            publisher = str(cl_df.iloc[i, 18]).strip() if len(cl_df.columns) > 18 else ''  # Såˆ—
            series = str(cl_df.iloc[i, 24]).strip() if len(cl_df.columns) > 24 else ''  # Yåˆ—

            if cno and cno != 'nan':
                cl_dict[cno] = {
                    'publisher': publisher if publisher != 'nan' else '',
                    'series': series if series != 'nan' else ''
                }
        except Exception:
            continue

    # is_list.csvã®å‡ºç‰ˆç¤¾ã¨ã‚·ãƒªãƒ¼ã‚ºã‚’ç½®æ›
    for i in range(1, len(is_df)):
        try:
            cno = str(is_df.iloc[i, 6]).strip() if len(is_df.columns) > 6 else ''  # Gåˆ—ï¼ˆã‚³ãƒŸãƒƒã‚¯Noï¼‰
            if cno in cl_dict:
                if cl_dict[cno]['publisher'] and len(is_df.columns) > 11:
                    is_df.iloc[i, 11] = cl_dict[cno]['publisher']  # Låˆ—
                if cl_dict[cno]['series'] and len(is_df.columns) > 13:
                    is_df.iloc[i, 13] = cl_dict[cno]['series']  # Nåˆ—
        except Exception:
            continue

    return is_df


def normalize_jan_code(value):
    """JANã‚³ãƒ¼ãƒ‰ã‚’æ­£è¦åŒ–ï¼ˆæ•°å€¤ã®.0é™¤å»ã€nané™¤å»ï¼‰"""
    if pd.isna(value):
        return ''
    jan_str = str(value).strip()
    # '.0' ã‚’é™¤å»ï¼ˆpandasã§æ•°å€¤ã¨ã—ã¦èª­ã¿è¾¼ã¾ã‚ŒãŸå ´åˆï¼‰
    if jan_str.endswith('.0'):
        jan_str = jan_str[:-2]
    # 'nan' ã¯ç©ºæ–‡å­—ã«
    if jan_str.lower() == 'nan':
        return ''
    return jan_str


def extract_first_volumes(merged_df):
    """1å·»ã®ã¿ã‚’æŠ½å‡ºã—ã¦æ•´å½¢"""
    first_vol_dict = {}
    latest_vol_dict = {}
    comic_info_dict = {}  # comic_noã”ã¨ã®æƒ…å ±ã‚’ä¿æŒ

    # ãƒ‘ã‚¹1: å…¨è¡Œã‚’å‡¦ç†ã—ã¦ first_vol_dict ã¨ latest_vol_dict ã‚’æ§‹ç¯‰
    for i in range(1, len(merged_df)):
        try:
            comic_no = normalize_jan_code(merged_df.iloc[i, 6]) if len(merged_df.columns) > 6 else ''  # Gåˆ—
            if not comic_no:
                continue

            # JANæƒ…å ±ï¼ˆæ­£è¦åŒ–ï¼‰
            jan_code = normalize_jan_code(merged_df.iloc[i, 5]) if len(merged_df.columns) > 5 else ''  # Fåˆ—
            if jan_code:
                latest_vol_dict[comic_no] = jan_code

            # 1å·»ãƒã‚§ãƒƒã‚¯ï¼ˆJåˆ—ï¼‰
            volume = str(merged_df.iloc[i, 9]).strip() if len(merged_df.columns) > 9 else ''
            if volume == '1' or volume == '1.0':
                if comic_no not in first_vol_dict and jan_code:
                    first_vol_dict[comic_no] = jan_code

            # comic_noã®æœ€åˆã®å‡ºç¾è¡Œã®æƒ…å ±ã‚’ä¿æŒ
            if comic_no not in comic_info_dict:
                comic_info_dict[comic_no] = {
                    'kaikatsu_narabi': str(merged_df.iloc[i, 3]).strip() if len(merged_df.columns) > 3 else '',
                    'first_isbn': str(merged_df.iloc[i, 4]).strip() if len(merged_df.columns) > 4 else '',
                    'comic_no': comic_no,
                    'genre': str(merged_df.iloc[i, 7]).strip() if len(merged_df.columns) > 7 else '',
                    'title': str(merged_df.iloc[i, 8]).strip() if len(merged_df.columns) > 8 else '',
                    'publisher': str(merged_df.iloc[i, 11]).strip() if len(merged_df.columns) > 11 else '',
                    'author': str(merged_df.iloc[i, 12]).strip() if len(merged_df.columns) > 12 else '',
                    'series': str(merged_df.iloc[i, 13]).strip() if len(merged_df.columns) > 13 else '',
                }
        except Exception:
            continue

    # ãƒ‘ã‚¹2: result_dataã‚’æ§‹ç¯‰ï¼ˆå…¨è¡Œå‡¦ç†å¾Œã«first_janã‚’è¨­å®šï¼‰
    result_data = []
    for comic_no, info in comic_info_dict.items():
        # 1å·»ã®JAN > æœ€æ–°å·»ã®JAN > ç©º ã®å„ªå…ˆé †ä½
        first_jan = first_vol_dict.get(comic_no, latest_vol_dict.get(comic_no, ''))
        info['first_jan'] = first_jan
        result_data.append(info)

    # å¿«æ´»ä¸¦ã³ã§ã‚½ãƒ¼ãƒˆ
    result_data.sort(key=lambda x: int(float(x['kaikatsu_narabi'])) if x['kaikatsu_narabi'] and x['kaikatsu_narabi'] != 'nan' else 999999)
    return result_data


def add_folder_hierarchy_info(result_data, hierarchy_df):
    """ãƒ•ã‚©ãƒ«ãƒ€éšå±¤æƒ…å ±ã‚’ä»˜ä¸"""
    hierarchy_list = []
    for i in range(1, len(hierarchy_df)):
        try:
            row = hierarchy_df.iloc[i]
            hierarchy_list.append({
                'genre': str(row[0]).strip() if pd.notna(row[0]) else '',
                'publisher': str(row[1]).strip() if pd.notna(row[1]) else '',
                'series': str(row[2]).strip() if len(row) > 2 and pd.notna(row[2]) else '',
                'main_folder': str(row[3]).strip() if len(row) > 3 and pd.notna(row[3]) else '',
                'sub_folder': str(row[4]).strip() if len(row) > 4 and pd.notna(row[4]) else ''
            })
        except Exception:
            continue

    for data in result_data:
        matched = False
        for h in hierarchy_list:
            if data['genre'] == h['genre'] and data['publisher'] == h['publisher']:
                if data['series'] and h['series']:
                    if data['series'] == h['series']:
                        data['main_folder'] = h['main_folder']
                        data['sub_folder'] = h['sub_folder']
                        matched = True
                        break
                elif not h['series']:
                    data['main_folder'] = h['main_folder']
                    data['sub_folder'] = h['sub_folder']
                    matched = True
                    break
        if not matched:
            data['main_folder'] = ''
            data['sub_folder'] = ''

    return result_data


def get_bookoff_image(jan_code, session):
    """ãƒ–ãƒƒã‚¯ã‚ªãƒ•ã‹ã‚‰ç”»åƒURLå–å¾—"""
    url = f"https://shopping.bookoff.co.jp/search/keyword/{jan_code}"
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}

    NO_IMAGE_PATTERNS = ['item_ll.gif', 'no_image', 'noimage', 'no-image', 'dummy', 'blank', 'spacer']
    BeautifulSoup = get_bs4()

    try:
        response = session.get(url, headers=headers, timeout=10)
        response.raise_for_status()

        soup = BeautifulSoup(response.content, 'html.parser')
        img_tag = soup.select_one('.productItem__image img, .js-gridImg')

        if img_tag and img_tag.get('src'):
            image_url = img_tag['src']
            if any(no_img in image_url.lower() for no_img in NO_IMAGE_PATTERNS):
                return None
            return image_url
        return None
    except Exception:
        return None


def get_amazon_image(jan_code, session):
    """Amazonã‹ã‚‰ç”»åƒURLå–å¾—ï¼ˆè¤‡æ•°ã‚»ãƒ¬ã‚¯ã‚¿å¯¾å¿œï¼‰"""
    search_url = f"https://www.amazon.co.jp/s?k={jan_code}&i=stripbooks"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
        'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
    }

    # è¤‡æ•°ã®ã‚»ãƒ¬ã‚¯ã‚¿ã‚’è©¦ã™ï¼ˆã‚µã‚¤ãƒˆæ§‹é€ å¤‰æ›´ã«å¯¾å¿œï¼‰
    SELECTORS = [
        '.s-image',
        'img[data-image-latency]',
        '.s-product-image img',
        '[data-component-type="s-product-image"] img',
        '.s-result-item img[src*="images-na"]',
        '.s-result-item img[src*="m.media-amazon"]',
    ]
    BeautifulSoup = get_bs4()

    try:
        response = session.get(search_url, headers=headers, timeout=15)
        if response.status_code == 503:
            return None
        response.raise_for_status()

        soup = BeautifulSoup(response.content, 'html.parser')

        # è¤‡æ•°ã®ã‚»ãƒ¬ã‚¯ã‚¿ã‚’é †ç•ªã«è©¦ã™
        for selector in SELECTORS:
            img_tags = soup.select(selector)
            for img_tag in img_tags:
                src = img_tag.get('src') or img_tag.get('data-src')
                if src and ('images-na' in src or 'm.media-amazon' in src or 'images-amazon' in src):
                    # NO IMAGEç³»ã‚’é™¤å¤–
                    if 'no-img' not in src.lower() and 'no_image' not in src.lower():
                        # é«˜è§£åƒåº¦ç‰ˆã«å¤‰æ›
                        if '_AC_' in src:
                            src = src.split('._AC_')[0] + '._SY466_.jpg'
                        elif '_SX' in src or '_SY' in src:
                            # ã‚µã‚¤ã‚ºæŒ‡å®šã‚’å¤§ããã™ã‚‹
                            import re
                            src = re.sub(r'\._S[XY]\d+_', '._SY466_', src)
                        return src

        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ­£è¦è¡¨ç¾ã§Amazonç”»åƒURLã‚’æ¢ã™
        import re
        amazon_img_pattern = r'(https?://[^"\']+(?:images-na\.ssl-images-amazon|m\.media-amazon|images-amazon)[^"\'\s]+\.(?:jpg|jpeg|png))'
        matches = re.findall(amazon_img_pattern, response.text)
        for match in matches:
            if 'no-img' not in match.lower() and 'no_image' not in match.lower() and 'sprite' not in match.lower():
                if '_AC_' in match:
                    match = match.split('._AC_')[0] + '._SY466_.jpg'
                return match

        return None
    except Exception:
        return None


def get_rakuten_image(jan_code, session):
    """æ¥½å¤©ãƒ–ãƒƒã‚¯ã‚¹ã‹ã‚‰ç”»åƒURLå–å¾—ï¼ˆAmazonã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
    search_url = f"https://books.rakuten.co.jp/search?g=001&isbn={jan_code}"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    }
    BeautifulSoup = get_bs4()

    try:
        response = session.get(search_url, headers=headers, timeout=10)
        response.raise_for_status()

        soup = BeautifulSoup(response.content, 'html.parser')

        # æ¥½å¤©ãƒ–ãƒƒã‚¯ã‚¹ã®ç”»åƒã‚»ãƒ¬ã‚¯ã‚¿
        selectors = [
            '.rbcomp__item-list__item__image img',
            '.item-image img',
            'img[src*="thumbnail.image.rakuten"]',
        ]

        for selector in selectors:
            img_tag = soup.select_one(selector)
            if img_tag:
                src = img_tag.get('src') or img_tag.get('data-src')
                if src and 'noimage' not in src.lower():
                    # å¤§ãã„ã‚µã‚¤ã‚ºã«å¤‰æ›
                    src = src.replace('_ex=64x64', '_ex=200x200').replace('_ex=100x100', '_ex=200x200')
                    return src

        return None
    except Exception:
        return None


def get_image_with_gemini_ai(jan_code, session, source_name="amazon"):
    """Gemini AIã‚’ä½¿ã£ã¦ç”»åƒURLã‚’æŠ½å‡ºï¼ˆã‚»ãƒ«ãƒ•ãƒ’ãƒ¼ãƒªãƒ³ã‚°æ©Ÿèƒ½ï¼‰"""
    # Geminiãƒ¢ãƒ‡ãƒ«ã‚’é…å»¶èª­ã¿è¾¼ã¿
    model = get_gemini_model()
    if not model:
        return None

    # ã‚½ãƒ¼ã‚¹åˆ¥ã®URLè¨­å®š
    if source_name == "amazon":
        search_url = f"https://www.amazon.co.jp/s?k={jan_code}&i=stripbooks"
    elif source_name == "rakuten":
        search_url = f"https://books.rakuten.co.jp/search?g=001&isbn={jan_code}"
    elif source_name == "bookoff":
        search_url = f"https://shopping.bookoff.co.jp/search/keyword/{jan_code}"
    else:
        return None

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    }
    BeautifulSoup = get_bs4()

    try:
        response = session.get(search_url, headers=headers, timeout=15)
        if response.status_code != 200:
            return None

        # HTMLã®é‡è¦éƒ¨åˆ†ã ã‘ã‚’æŠ½å‡ºï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ç¯€ç´„ï¼‰
        soup = BeautifulSoup(response.content, 'html.parser')

        # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨ã‚¹ã‚¿ã‚¤ãƒ«ã‚’å‰Šé™¤
        for tag in soup(['script', 'style', 'noscript', 'header', 'footer', 'nav']):
            tag.decompose()

        # å•†å“ç”»åƒãŒå«ã¾ã‚Œãã†ãªéƒ¨åˆ†ã‚’æŠ½å‡º
        main_content = soup.find('main') or soup.find('div', {'id': 'search'}) or soup.find('body')
        if main_content:
            html_snippet = str(main_content)[:8000]  # æœ€å¤§8000æ–‡å­—ã«åˆ¶é™
        else:
            html_snippet = str(soup)[:8000]

        prompt = f"""ä»¥ä¸‹ã®HTMLã‹ã‚‰ã€JANã‚³ãƒ¼ãƒ‰ã€Œ{jan_code}ã€ã®æœ¬ã®è¡¨ç´™ç”»åƒURLã‚’1ã¤ã ã‘æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

æ¡ä»¶:
- ç”»åƒURLã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ï¼ˆèª¬æ˜ä¸è¦ï¼‰
- NO IMAGEã€noimageã€placeholderç­‰ã®ãƒ€ãƒŸãƒ¼ç”»åƒã¯é™¤å¤–
- https://ã§å§‹ã¾ã‚‹å®Œå…¨ãªURLã§è¿”ã—ã¦ãã ã•ã„
- è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€ŒNOT_FOUNDã€ã¨ã ã‘è¿”ã—ã¦ãã ã•ã„

HTML:
{html_snippet}"""

        response = model.generate_content(prompt)
        result = response.text.strip()

        # çµæœã‚’æ¤œè¨¼
        if result and result != "NOT_FOUND" and result.startswith("http"):
            # NO IMAGEç³»ã‚’æœ€çµ‚ãƒã‚§ãƒƒã‚¯
            no_image_patterns = ['no_image', 'noimage', 'no-image', 'dummy', 'blank', 'spacer', 'placeholder']
            if not any(p in result.lower() for p in no_image_patterns):
                return result

        return None

    except Exception as e:
        # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
        print(f"Gemini AI error: {e}")
        return None


def download_image(image_url, session):
    """ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™ï¼ˆNO IMAGEæ¤œå‡ºä»˜ãï¼‰"""
    try:
        response = session.get(image_url, timeout=10)
        response.raise_for_status()
        content = response.content

        # ç”»åƒã‚µã‚¤ã‚ºãŒå°ã•ã™ãã‚‹å ´åˆã¯NO IMAGEã®å¯èƒ½æ€§ãŒé«˜ã„ï¼ˆ5KBæœªæº€ï¼‰
        if len(content) < 5000:
            return None

        # ç‰¹å®šã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’URLã§å†ãƒã‚§ãƒƒã‚¯
        no_image_patterns = ['no_image', 'noimage', 'no-image', 'dummy', 'blank', 'spacer', 'placeholder']
        if any(pattern in image_url.lower() for pattern in no_image_patterns):
            return None

        return content
    except Exception:
        return None


@st.cache_data(ttl=600, show_spinner=False)
def get_all_folders():
    """R-Cabinetã®å…¨ãƒ•ã‚©ãƒ«ãƒ€ä¸€è¦§ã‚’å–å¾—"""
    url = f"{BASE_URL}/cabinet/folders/get"
    headers = get_auth_header()

    all_folders = []
    offset = 1  # 1å§‹ã¾ã‚Šï¼ˆãƒšãƒ¼ã‚¸ç•ªå·ï¼‰
    limit = 100  # APIã®ä¸Šé™ã¯100ä»¶

    while True:
        params = {"offset": offset, "limit": limit}

        try:
            response = requests.get(url, headers=headers, params=params, timeout=30)
        except requests.exceptions.RequestException as e:
            return None, f"æ¥ç¶šã‚¨ãƒ©ãƒ¼: {str(e)}"

        if response.status_code != 200:
            return None, f"ã‚¨ãƒ©ãƒ¼: {response.status_code} - {response.text[:200]}"

        try:
            root = ET.fromstring(response.text)
        except ET.ParseError as e:
            return None, f"XMLãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {str(e)}"

        # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
        system_status = root.findtext('.//systemStatus', '')
        if system_status != 'OK':
            message = root.findtext('.//message', 'Unknown error')
            return None, f"APIã‚¨ãƒ©ãƒ¼: {message}"

        folders = root.findall('.//folder')

        for folder in folders:
            all_folders.append({
                'FolderId': folder.findtext('FolderId', ''),
                'FolderName': folder.findtext('FolderName', ''),
                'FolderPath': folder.findtext('FolderPath', ''),
                'FileCount': safe_int(folder.findtext('FileCount', '0')),
            })

        # å–å¾—ä»¶æ•°ãŒlimitæœªæº€ãªã‚‰çµ‚äº†ï¼ˆæœ€çµ‚ãƒšãƒ¼ã‚¸ï¼‰
        if len(folders) < limit:
            break
        offset += 1  # æ¬¡ã®ãƒšãƒ¼ã‚¸ã¸
        time.sleep(0.3)

    return all_folders, None


@st.cache_data(ttl=300, show_spinner=False)
def get_folder_files(folder_id: int, max_retries: int = 3):
    """æŒ‡å®šãƒ•ã‚©ãƒ«ãƒ€å†…ã®ç”»åƒä¸€è¦§ã‚’å–å¾—ï¼ˆãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãï¼‰"""
    url = f"{BASE_URL}/cabinet/folder/files/get"
    headers = get_auth_header()

    all_files = []
    offset = 1  # 1å§‹ã¾ã‚Šï¼ˆãƒšãƒ¼ã‚¸ç•ªå·ï¼‰
    limit = 100  # APIã®ä¸Šé™ã¯100ä»¶

    while True:
        params = {"folderId": folder_id, "offset": offset, "limit": limit}

        # ãƒªãƒˆãƒ©ã‚¤å‡¦ç†
        for retry in range(max_retries):
            try:
                response = requests.get(url, headers=headers, params=params, timeout=30)
            except requests.exceptions.RequestException as e:
                if retry < max_retries - 1:
                    time.sleep(2)  # 2ç§’å¾…ã£ã¦ãƒªãƒˆãƒ©ã‚¤
                    continue
                return None, f"æ¥ç¶šã‚¨ãƒ©ãƒ¼: {str(e)}"

            if response.status_code == 200:
                break  # æˆåŠŸ
            elif response.status_code == 403 and retry < max_retries - 1:
                time.sleep(3)  # 403ã®å ´åˆã¯3ç§’å¾…ã£ã¦ãƒªãƒˆãƒ©ã‚¤
                continue
            else:
                if retry == max_retries - 1:
                    return None, f"ã‚¨ãƒ©ãƒ¼: {response.status_code}"

        try:
            root = ET.fromstring(response.text)
        except ET.ParseError as e:
            return None, f"XMLãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {str(e)}"

        system_status = root.findtext('.//systemStatus', '')
        if system_status != 'OK':
            message = root.findtext('.//message', 'Unknown error')
            return None, f"APIã‚¨ãƒ©ãƒ¼: {message}"

        files = root.findall('.//file')

        for f in files:
            all_files.append({
                'FileId': f.findtext('FileId', ''),
                'FileName': f.findtext('FileName', ''),
                'FileUrl': f.findtext('FileUrl', ''),
                'FilePath': f.findtext('FilePath', ''),
                'FileSize': f.findtext('FileSize', ''),
                'TimeStamp': f.findtext('TimeStamp', ''),
            })

        # å–å¾—ä»¶æ•°ãŒlimitæœªæº€ãªã‚‰çµ‚äº†ï¼ˆæœ€çµ‚ãƒšãƒ¼ã‚¸ï¼‰
        if len(files) < limit:
            break
        offset += 1  # æ¬¡ã®ãƒšãƒ¼ã‚¸ã¸
        time.sleep(0.3)

    return all_files, None


def search_image_by_name(file_name: str):
    """ç”»åƒåã§æ¤œç´¢"""
    url = f"{BASE_URL}/cabinet/files/search"
    headers = get_auth_header()
    params = {"fileName": file_name}

    response = requests.get(url, headers=headers, params=params)

    if response.status_code == 200:
        root = ET.fromstring(response.text)
        files = root.findall('.//file')

        results = []
        for f in files:
            results.append({
                'FileId': f.findtext('FileId', ''),
                'FileName': f.findtext('FileName', ''),
                'FileUrl': f.findtext('FileUrl', ''),
                'FolderName': f.findtext('FolderName', ''),
                'FolderPath': f.findtext('FolderPath', ''),
            })
        return results
    return []


def is_exact_match(file_name: str, comic_no: str) -> bool:
    """ãƒ•ã‚¡ã‚¤ãƒ«åãŒã‚³ãƒŸãƒƒã‚¯Noã¨å®Œå…¨ä¸€è‡´ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆæ‹¡å¼µå­é™¤ãï¼‰"""
    # æ‹¡å¼µå­ã‚’é™¤å»
    name_without_ext = file_name.rsplit('.', 1)[0] if '.' in file_name else file_name
    # å®Œå…¨ä¸€è‡´ã®ã¿
    return name_without_ext == comic_no


def check_comic_images(comic_numbers: list, progress_bar=None, status_text=None):
    """ã‚³ãƒŸãƒƒã‚¯Noãƒªã‚¹ãƒˆã®ç”»åƒå­˜åœ¨ãƒã‚§ãƒƒã‚¯"""
    results = []
    total = len(comic_numbers)

    for i, comic_no in enumerate(comic_numbers):
        if progress_bar:
            progress_bar.progress((i + 1) / total)
        if status_text:
            status_text.text(f"ãƒã‚§ãƒƒã‚¯ä¸­: {comic_no} ({i + 1}/{total})")

        found_files = search_image_by_name(str(comic_no))

        # å®Œå…¨ä¸€è‡´ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        matched_files = [f for f in found_files if is_exact_match(f['FileName'], str(comic_no))]

        if matched_files:
            for f in matched_files:
                results.append({
                    'ã‚³ãƒŸãƒƒã‚¯No': comic_no,
                    'å­˜åœ¨': 'âœ… ã‚ã‚Š',
                    'ãƒ•ã‚¡ã‚¤ãƒ«å': f['FileName'],
                    'ãƒ•ã‚©ãƒ«ãƒ€': f['FolderName'],
                    'URL': f['FileUrl'],
                })
        else:
            results.append({
                'ã‚³ãƒŸãƒƒã‚¯No': comic_no,
                'å­˜åœ¨': 'âŒ ãªã—',
                'ãƒ•ã‚¡ã‚¤ãƒ«å': '-',
                'ãƒ•ã‚©ãƒ«ãƒ€': '-',
                'URL': '-',
            })

        time.sleep(0.4)

    return results


# èªè¨¼æƒ…å ±ãƒã‚§ãƒƒã‚¯
if not SERVICE_SECRET or not LICENSE_KEY:
    st.error("âš ï¸ RMS APIèªè¨¼æƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚Streamlit Secretsã«è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    st.stop()


# ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šãƒ¢ãƒ¼ãƒ‰åˆ‡æ›¿
with st.sidebar:
    st.title("ğŸ–¼ï¸ R-Cabinet")
    st.caption(f"v{APP_VERSION}")

    st.markdown("<br>", unsafe_allow_html=True)

    mode = st.radio(
        "æ©Ÿèƒ½ã‚’é¸æŠ",
        ["ğŸ“‚ ç”»åƒä¸€è¦§å–å¾—", "ğŸ” ç”»åƒå­˜åœ¨ãƒã‚§ãƒƒã‚¯", "ğŸ“¥ ä¸è¶³ç”»åƒå–å¾—"],
        label_visibility="collapsed"
    )

    st.markdown("<br>", unsafe_allow_html=True)
    st.divider()
    st.markdown("<br>", unsafe_allow_html=True)


# ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
if mode == "ğŸ“‚ ç”»åƒä¸€è¦§å–å¾—":
    st.title("ğŸ“‚ ç”»åƒä¸€è¦§å–å¾—")
    st.markdown("R-Cabinetã®ãƒ•ã‚©ãƒ«ãƒ€ã‚’é¸æŠã—ã¦ã€ç”»åƒã‚’ä¸€è¦§è¡¨ç¤ºã—ã¾ã™ã€‚")

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
    if "folders_loaded" not in st.session_state:
        st.session_state.folders_loaded = False
        st.session_state.folders_data = None
        st.session_state.folders_error = None
    if "images_loaded" not in st.session_state:
        st.session_state.images_loaded = False
        st.session_state.images_data = None

    # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ•ã‚©ãƒ«ãƒ€ä¸€è¦§ã‚’å–å¾—ï¼ˆã¾ã ã®å ´åˆï¼‰
    if not st.session_state.folders_loaded:
        st.markdown("### ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ•ã‚©ãƒ«ãƒ€ä¸€è¦§ã‚’å–å¾—")
        if st.button("ğŸ“‚ ãƒ•ã‚©ãƒ«ãƒ€ä¸€è¦§ã‚’å–å¾—", type="primary"):
            with st.spinner("ãƒ•ã‚©ãƒ«ãƒ€ä¸€è¦§ã‚’å–å¾—ä¸­..."):
                folders, error = get_all_folders()
            st.session_state.folders_data = folders
            st.session_state.folders_error = error
            st.session_state.folders_loaded = True
            st.rerun()
        st.stop()

    folders = st.session_state.folders_data
    error = st.session_state.folders_error

    if error:
        st.error(error)
        if st.button("ğŸ”„ å†è©¦è¡Œ"):
            st.session_state.folders_loaded = False
            st.cache_data.clear()
            st.rerun()
        st.stop()

    if not folders:
        st.warning("ãƒ•ã‚©ãƒ«ãƒ€ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        st.stop()

    # ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’è¨ˆç®—
    total_files = sum(f['FileCount'] for f in folders)

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«ãƒ•ã‚©ãƒ«ãƒ€æƒ…å ±
    with st.sidebar:
        st.success(f"ğŸ“ {len(folders)} ãƒ•ã‚©ãƒ«ãƒ€")
        st.info(f"ğŸ“· {total_files} ç”»åƒï¼ˆå…¨ä½“ï¼‰")
        if st.button("ğŸ”„ ãƒ•ã‚©ãƒ«ãƒ€å†å–å¾—"):
            st.session_state.folders_loaded = False
            st.session_state.images_loaded = False
            st.cache_data.clear()
            st.rerun()

    # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ•ã‚©ãƒ«ãƒ€é¸æŠ
    st.markdown("### ãƒ•ã‚©ãƒ«ãƒ€ã‚’é¸æŠ")

    folder_options = {"ğŸ“ ã™ã¹ã¦ï¼ˆå…¨ãƒ•ã‚©ãƒ«ãƒ€ï¼‰": None}
    folder_options.update({f"{f['FolderName']} ({f['FileCount']}ä»¶)": f for f in folders})

    selected_folder_name = st.selectbox(
        "å–å¾—ã™ã‚‹ãƒ•ã‚©ãƒ«ãƒ€",
        list(folder_options.keys()),
        label_visibility="collapsed"
    )

    # DBçµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
    db_stats = get_db_stats()
    if db_stats.get("total", 0) > 0:
        stat_cols = st.columns(4)
        with stat_cols[0]:
            st.metric("DBç™»éŒ²æ•°", db_stats.get("total", 0))
        with stat_cols[1]:
            st.metric("é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«", db_stats.get("duplicates", 0))
        with stat_cols[2]:
            st.metric("APIç·æ•°", total_files)
        with stat_cols[3]:
            last_updated = db_stats.get("last_updated", "-")
            st.metric("æœ€çµ‚æ›´æ–°", last_updated if last_updated else "-")

    # ã‚¹ãƒ†ãƒƒãƒ—3: æ“ä½œãƒœã‚¿ãƒ³ï¼ˆ2ã¤ï¼‰
    btn_col1, btn_col2, _ = st.columns([1.2, 1.2, 2])
    with btn_col1:
        show_db_btn = st.button(
            "ğŸ“‚ ä¿å­˜æ¸ˆã¿ä¸€è¦§ã‚’è¡¨ç¤º",
            disabled=(db_stats.get("total", 0) == 0),
            help="DBã«ä¿å­˜ã•ã‚ŒãŸä¸€è¦§ã‚’è¡¨ç¤ºï¼ˆé«˜é€Ÿï¼‰"
        )
    with btn_col2:
        fetch_api_btn = st.button(
            "ğŸ”„ æœ€æ–°ä¸€è¦§ã‚’å–å¾—",
            type="primary",
            help="APIã‹ã‚‰æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦DBã«åŒæœŸ"
        )

    st.divider()

    # ãƒœã‚¿ãƒ³æŠ¼ä¸‹æ™‚ã®å‡¦ç†
    if show_db_btn:
        # DBã‹ã‚‰èª­ã¿è¾¼ã¿
        st.session_state.data_source = "db"
        if selected_folder_name == "ğŸ“ ã™ã¹ã¦ï¼ˆå…¨ãƒ•ã‚©ãƒ«ãƒ€ï¼‰":
            loaded_images, msg = load_images_from_db()
        else:
            folder_name = folder_options[selected_folder_name]['FolderName']
            loaded_images = load_images_from_db_by_folder(folder_name)
            msg = f"{len(loaded_images)}ä»¶ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ"

        if loaded_images:
            st.session_state.images_data = loaded_images
            st.session_state.images_loaded = True
            st.session_state.error_folders = []
            st.success(f"ğŸ“‚ DBã‹ã‚‰{msg}")
        else:
            st.warning("DBã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

    if fetch_api_btn:
        # APIã‹ã‚‰å–å¾—ã—ã¦DBåŒæœŸ
        st.session_state.data_source = "api"
        st.session_state.images_loaded = False
        st.session_state.images_data = None

        if selected_folder_name == "ğŸ“ ã™ã¹ã¦ï¼ˆå…¨ãƒ•ã‚©ãƒ«ãƒ€ï¼‰":
            # å…¨ãƒ•ã‚©ãƒ«ãƒ€ã®ç”»åƒã‚’å–å¾—
            all_files = []
            error_folders = []
            expected_total = sum(f['FileCount'] for f in folders)
            progress_bar = st.progress(0)
            status_text = st.empty()

            for i, folder in enumerate(folders):
                status_text.text(f"å–å¾—ä¸­: {folder['FolderName']} ({i + 1}/{len(folders)}) - {folder['FileCount']}ä»¶")
                progress_bar.progress((i + 1) / len(folders))

                files, err = get_folder_files(int(folder['FolderId']))
                time.sleep(0.5)

                if err:
                    error_folders.append({
                        'FolderName': folder['FolderName'],
                        'FolderId': folder['FolderId'],
                        'FileCount': folder['FileCount'],
                        'Error': err
                    })
                if files:
                    for f in files:
                        f['FolderName'] = folder['FolderName']
                    all_files.extend(files)

            progress_bar.empty()
            status_text.empty()

            # DBåŒæœŸ
            with st.spinner("DBã«åŒæœŸä¸­..."):
                sync_result = sync_images_to_db(all_files)

            if sync_result.get("success"):
                st.success(f"ğŸ”„ APIå–å¾—å®Œäº†ãƒ»DBåŒæœŸæ¸ˆã¿ï¼ˆæ–°è¦: {sync_result['new']} / æ›´æ–°: {sync_result['updated']} / é‡è¤‡: {sync_result['duplicate']}ï¼‰")
                if sync_result['duplicate'] > 0:
                    st.warning(f"âš ï¸ {sync_result['duplicate']}ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¤‡æ•°ãƒ•ã‚©ãƒ«ãƒ€ã«å­˜åœ¨")
            else:
                st.error(f"DBåŒæœŸã‚¨ãƒ©ãƒ¼: {sync_result.get('error')}")

            st.session_state.images_data = all_files
            st.session_state.error_folders = error_folders
            st.session_state.expected_total = expected_total
            st.session_state.images_loaded = True
        else:
            # å€‹åˆ¥ãƒ•ã‚©ãƒ«ãƒ€ã®å ´åˆ
            selected_folder = folder_options[selected_folder_name]
            folder_id = int(selected_folder['FolderId'])

            with st.spinner(f"ã€Œ{selected_folder['FolderName']}ã€ã®ç”»åƒã‚’å–å¾—ä¸­..."):
                files, error = get_folder_files(folder_id)

            if error:
                st.error(error)
            elif files:
                for f in files:
                    f['FolderName'] = selected_folder['FolderName']

                # DBåŒæœŸ
                with st.spinner("DBã«åŒæœŸä¸­..."):
                    sync_result = sync_images_to_db(files)

                if sync_result.get("success"):
                    st.success(f"ğŸ”„ å–å¾—å®Œäº†ï¼ˆ{len(files)}ä»¶ï¼‰ãƒ»DBåŒæœŸæ¸ˆã¿")

                st.session_state.images_data = files
                st.session_state.error_folders = []
                st.session_state.images_loaded = True

    # ç”»åƒä¸€è¦§è¡¨ç¤º
    if st.session_state.images_loaded and st.session_state.images_data:
        all_files = st.session_state.images_data
        error_folders = st.session_state.get('error_folders', [])

        if all_files:
            # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
            st.success(f"ğŸ“· {len(all_files)} ä»¶ã®ç”»åƒ")

            # ã‚¨ãƒ©ãƒ¼ãƒ•ã‚©ãƒ«ãƒ€ãŒã‚ã‚Œã°è¡¨ç¤º
            if error_folders:
                with st.expander(f"âš ï¸ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸãƒ•ã‚©ãƒ«ãƒ€ ({len(error_folders)}ä»¶)", expanded=False):
                    for ef in error_folders:
                        st.write(f"- **{ef['FolderName']}** ({ef['FileCount']}ä»¶): {ef['Error']}")

            # æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
            search_term = st.text_input("ğŸ” ãƒ•ã‚¡ã‚¤ãƒ«åã§çµã‚Šè¾¼ã¿", placeholder="æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰")

            display_files = all_files
            if search_term:
                display_files = [f for f in all_files if search_term.lower() in f['FileName'].lower()]
                st.info(f"çµã‚Šè¾¼ã¿çµæœ: {len(display_files)} ä»¶")

            # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ è¡¨ç¤º
            df = pd.DataFrame(display_files)
            df = df[['FolderName', 'FileName', 'FileUrl', 'FileSize', 'TimeStamp']]
            df.columns = ['ãƒ•ã‚©ãƒ«ãƒ€', 'ãƒ•ã‚¡ã‚¤ãƒ«å', 'URL', 'ã‚µã‚¤ã‚º(KB)', 'æ›´æ–°æ—¥æ™‚']

            st.dataframe(df, use_container_width=True, height=500)

            # Excelãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            excel_buffer = BytesIO()
            with pd.ExcelWriter(excel_buffer, engine='openpyxl') as writer:
                df.to_excel(writer, index=False, sheet_name='Sheet1')
                style_excel(writer.sheets['Sheet1'], num_columns=5, url_column=3)
            excel_buffer.seek(0)
            st.download_button(
                label="ğŸ“¥ Excelã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=excel_buffer,
                file_name="rcabinet_images.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )
        else:
            st.warning("ç”»åƒãŒã‚ã‚Šã¾ã›ã‚“ã€‚")


elif mode == "ğŸ” ç”»åƒå­˜åœ¨ãƒã‚§ãƒƒã‚¯":
    st.title("ğŸ” ç”»åƒå­˜åœ¨ãƒã‚§ãƒƒã‚¯")
    st.markdown("ã‚³ãƒŸãƒƒã‚¯Noã‚’å…¥åŠ›ã—ã¦ã€R-Cabinetã«ç”»åƒãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèªã—ã¾ã™ã€‚")

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
    if "check_results" not in st.session_state:
        st.session_state.check_results = None

    st.divider()

    # å…¥åŠ›æ–¹æ³•ã®é¸æŠ
    input_method = st.radio(
        "å…¥åŠ›æ–¹æ³•ã‚’é¸æŠ",
        ["ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›", "CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"],
        horizontal=True
    )

    comic_numbers = []

    if input_method == "ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›":
        st.markdown("### ã‚³ãƒŸãƒƒã‚¯Noå…¥åŠ›")
        st.markdown("1è¡Œã«1ã¤ã®ã‚³ãƒŸãƒƒã‚¯Noã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

        text_input = st.text_area(
            "ã‚³ãƒŸãƒƒã‚¯Noï¼ˆæ”¹è¡ŒåŒºåˆ‡ã‚Šï¼‰",
            height=200,
            placeholder="123456\n234567\n345678"
        )

        if text_input:
            comic_numbers = [line.strip() for line in text_input.split('\n') if line.strip()]
            st.info(f"å…¥åŠ›ã•ã‚ŒãŸã‚³ãƒŸãƒƒã‚¯No: {len(comic_numbers)}ä»¶")

    else:
        st.markdown("### CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
        st.markdown("ã‚³ãƒŸãƒƒã‚¯Noåˆ—ã‚’å«ã‚€CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")

        uploaded_file = st.file_uploader("CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ", type=['csv'])

        if uploaded_file:
            try:
                df = pd.read_csv(uploaded_file, encoding='utf-8')
            except:
                df = pd.read_csv(uploaded_file, encoding='cp932')

            st.markdown("#### ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
            st.dataframe(df.head(10), use_container_width=True)

            columns = df.columns.tolist()
            selected_column = st.selectbox("ã‚³ãƒŸãƒƒã‚¯Noåˆ—ã‚’é¸æŠ", columns, index=0)

            if selected_column:
                comic_numbers = df[selected_column].dropna().astype(str).tolist()
                st.info(f"èª­ã¿è¾¼ã‚“ã ã‚³ãƒŸãƒƒã‚¯No: {len(comic_numbers)}ä»¶")

    st.divider()

    # ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œãƒœã‚¿ãƒ³ï¼ˆå¸¸ã«è¡¨ç¤ºï¼‰
    check_button = st.button("ğŸ” ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ", type="primary")

    if check_button:
        if not comic_numbers:
            st.warning("ã‚³ãƒŸãƒƒã‚¯Noã‚’å…¥åŠ›ã¾ãŸã¯CSVã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        else:
            progress_bar = st.progress(0)
            status_text = st.empty()

            results = check_comic_images(comic_numbers, progress_bar, status_text)

            progress_bar.empty()
            status_text.empty()

            # çµæœã‚’session_stateã«ä¿å­˜
            st.session_state.check_results = results

    # çµæœè¡¨ç¤ºï¼ˆsession_stateã‹ã‚‰ï¼‰
    if st.session_state.check_results:
        results = st.session_state.check_results
        df_results = pd.DataFrame(results)

        st.markdown("### ãƒã‚§ãƒƒã‚¯çµæœ")

        exists_count = len([r for r in results if r['å­˜åœ¨'] == 'âœ… ã‚ã‚Š'])
        not_exists_count = len([r for r in results if r['å­˜åœ¨'] == 'âŒ ãªã—'])

        col1, col2, col3 = st.columns(3)
        col1.metric("ç·æ•°", len(results))
        col2.metric("å­˜åœ¨ã‚ã‚Š", exists_count)
        col3.metric("å­˜åœ¨ãªã—", not_exists_count)

        st.divider()

        filter_option = st.radio(
            "è¡¨ç¤ºãƒ•ã‚£ãƒ«ã‚¿ãƒ¼",
            ["ã™ã¹ã¦", "å­˜åœ¨ã‚ã‚Š", "å­˜åœ¨ãªã—"],
            horizontal=True
        )

        if filter_option == "å­˜åœ¨ã‚ã‚Š":
            df_display = df_results[df_results['å­˜åœ¨'] == 'âœ… ã‚ã‚Š']
        elif filter_option == "å­˜åœ¨ãªã—":
            df_display = df_results[df_results['å­˜åœ¨'] == 'âŒ ãªã—']
        else:
            df_display = df_results

        st.dataframe(df_display, use_container_width=True, height=400)

        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ï¼ˆ1è¡Œç›®ï¼šå·¦å¯„ã›ï¼‰
        dl_col1, dl_col2, _ = st.columns([1, 1.5, 2])

        with dl_col1:
            # Comic Searchæ¤œç´¢ç”¨CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆå­˜åœ¨ãªã—ã®ã‚³ãƒŸãƒƒã‚¯Noã®ã¿ï¼‰
            not_exists_comics = [r['ã‚³ãƒŸãƒƒã‚¯No'] for r in results if r['å­˜åœ¨'] == 'âŒ ãªã—']
            if not_exists_comics:
                # list_ã‚³ãƒŸãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼.csvå½¢å¼ã§ä½œæˆ
                is_csv_data = []
                for comic_no in not_exists_comics:
                    is_csv_data.append({
                        'ã‚¸ãƒ£ãƒ³ãƒ«': '',
                        'ã‚¿ã‚¤ãƒˆãƒ«': '',
                        'å‡ºç‰ˆç¤¾': '',
                        'è‘—è€…': '',
                        'å®Œçµ': '',
                        'å·»æ•°': '',
                        'ï¼©ï¼³ï¼¢ï¼®': '',
                        'æ£šç•ª': '',
                        'ã‚³ãƒ¡ãƒ³ãƒˆ': '',
                        'ã‚³ãƒŸâ„–': comic_no,
                        'å†Šæ•°': '1'
                    })
                df_is_csv = pd.DataFrame(is_csv_data)
                csv_buffer = BytesIO()
                df_is_csv.to_csv(csv_buffer, index=False, encoding='cp932')
                csv_buffer.seek(0)
                st.download_button(
                    label="ğŸ“¥ Comic Searchæ¤œç´¢ç”¨CSV",
                    data=csv_buffer,
                    file_name="list_ã‚³ãƒŸãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼.csv",
                    mime="text/csv"
                )
            else:
                st.button("ğŸ“¥ Comic Searchæ¤œç´¢ç”¨CSV", disabled=True, help="å­˜åœ¨ãªã—ã®ã‚³ãƒŸãƒƒã‚¯NoãŒã‚ã‚Šã¾ã›ã‚“")

        with dl_col2:
            # Excelãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚¹ã‚¿ã‚¤ãƒ«ä»˜ãï¼‰
            excel_buffer = BytesIO()
            with pd.ExcelWriter(excel_buffer, engine='openpyxl') as writer:
                df_results.to_excel(writer, index=False, sheet_name='Sheet1')
                style_excel(writer.sheets['Sheet1'], num_columns=5, url_column=5)
            excel_buffer.seek(0)
            st.download_button(
                label="ğŸ“¥ çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’Excelã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=excel_buffer,
                file_name="rcabinet_check_result.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )

        # 2è¡Œç›®ï¼šGitHubã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã€çµæœã‚¯ãƒªã‚¢
        btn_col3, btn_col4, _ = st.columns([1.5, 1, 2])

        with btn_col3:
            # GitHubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
            if not_exists_comics:
                if st.button("ğŸ“¤ GitHubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", help="ã‚³ãƒŸãƒƒã‚¯ãƒªã‚¹ã‚¿ãƒ¼ç”¨ã«GitHubã¸ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"):
                    # ã‚³ãƒŸãƒƒã‚¯ãƒªã‚¹ã‚¿ãƒ¼ç”¨CSVå½¢å¼ï¼ˆJåˆ—ã«ã‚³ãƒŸãƒƒã‚¯No.ã€Kåˆ—ã«1ï¼‰
                    csv_lines = []
                    for comic_no in not_exists_comics:
                        row = [''] * 9 + [str(comic_no), '1']
                        csv_lines.append(','.join(row))
                    csv_content = '\n'.join(csv_lines)

                    with st.spinner("GitHubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­..."):
                        today = datetime.now().strftime("%Y-%m-%d %H:%M")
                        result = upload_to_github(
                            csv_content,
                            GITHUB_MISSING_CSV_PATH,
                            f"Update missing_comics.csv ({len(not_exists_comics)}ä»¶) - {today}"
                        )

                    if result.get("success"):
                        st.success(f"GitHubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†ï¼ˆ{len(not_exists_comics)}ä»¶ï¼‰")
                    else:
                        st.error(f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {result.get('error')}")
            else:
                st.button("ğŸ“¤ GitHubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", disabled=True, help="å­˜åœ¨ãªã—ã®ã‚³ãƒŸãƒƒã‚¯NoãŒã‚ã‚Šã¾ã›ã‚“")

        with btn_col4:
            # çµæœã‚¯ãƒªã‚¢ãƒœã‚¿ãƒ³
            if st.button("ğŸ—‘ï¸ çµæœã‚’ã‚¯ãƒªã‚¢"):
                st.session_state.check_results = None
                st.rerun()


elif mode == "ğŸ“¥ ä¸è¶³ç”»åƒå–å¾—":
    st.title("ğŸ“¥ ä¸è¶³ç”»åƒå–å¾—")
    st.markdown("ISæ¤œç´¢çµæœã‹ã‚‰JANã‚³ãƒ¼ãƒ‰ã§ç”»åƒã‚’å–å¾—ã—ã€ZIPã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚")

    st.divider()

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
    if "github_is_list" not in st.session_state:
        st.session_state.github_is_list = None
    if "github_comic_list" not in st.session_state:
        st.session_state.github_comic_list = None
    if "github_folder_hierarchy" not in st.session_state:
        st.session_state.github_folder_hierarchy = None
    if "image_download_result" not in st.session_state:
        st.session_state.image_download_result = None

    st.markdown("### ã‚¹ãƒ†ãƒƒãƒ—0: GitHubã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—")
    st.markdown("GitHub Actionsã§ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—ã—ã¾ã™ã€‚")

    # è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ•ãƒ©ã‚°ï¼ˆç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢ï¼‰
    if "auto_download_tried" not in st.session_state:
        st.session_state.auto_download_tried = False

    # ã¾ã ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«èª­ã¿è¾¼ã¾ã‚Œã¦ã„ãªã„å ´åˆã¯è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆ1å›ã ã‘è©¦è¡Œï¼‰
    not_loaded_yet = not st.session_state.github_is_list or not st.session_state.github_comic_list or not st.session_state.github_folder_hierarchy

    if not_loaded_yet and not st.session_state.auto_download_tried:
        st.session_state.auto_download_tried = True
        with st.spinner("GitHubã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•å–å¾—ä¸­..."):
            auto_errors = []
            if not st.session_state.github_is_list:
                result = download_from_github(GITHUB_IS_LIST_PATH)
                if result.get("success"):
                    st.session_state.github_is_list = result["content"]
                else:
                    auto_errors.append(f"is_list.csv: {result.get('error', 'ä¸æ˜')}")
            if not st.session_state.github_comic_list:
                result = download_from_github(GITHUB_COMIC_LIST_PATH)
                if result.get("success"):
                    st.session_state.github_comic_list = result["content"]
                else:
                    auto_errors.append(f"comic_list.csv: {result.get('error', 'ä¸æ˜')}")
            if not st.session_state.github_folder_hierarchy:
                result = download_from_github(GITHUB_FOLDER_HIERARCHY_PATH)
                if result.get("success"):
                    st.session_state.github_folder_hierarchy = result["content"]
                else:
                    auto_errors.append(f"ãƒ•ã‚©ãƒ«ãƒ€éšå±¤ãƒªã‚¹ãƒˆ: {result.get('error', 'ä¸æ˜')}")
            if auto_errors:
                st.warning(f"è‡ªå‹•å–å¾—ã‚¨ãƒ©ãƒ¼: {', '.join(auto_errors)}")
        st.rerun()

    # GitHubãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’å–å¾—ï¼ˆè¡¨ç¤ºç”¨ï¼‰
    is_info = get_github_file_info(GITHUB_IS_LIST_PATH)
    cl_info = get_github_file_info(GITHUB_COMIC_LIST_PATH)
    fh_info = get_github_file_info(GITHUB_FOLDER_HIERARCHY_PATH)

    # GitHubãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’è¡¨ç¤º
    col_info1, col_info2, col_info3 = st.columns(3)
    with col_info1:
        if is_info.get("exists"):
            st.success(f"is_list.csv\næ›´æ–°: {is_info.get('last_updated', 'ä¸æ˜')}")
        else:
            st.warning("is_list.csv\næœªç”Ÿæˆ")
    with col_info2:
        if cl_info.get("exists"):
            st.success(f"comic_list.csv\næ›´æ–°: {cl_info.get('last_updated', 'ä¸æ˜')}")
        else:
            st.warning("comic_list.csv\næœªç”Ÿæˆ")
    with col_info3:
        if fh_info.get("exists"):
            st.success(f"ãƒ•ã‚©ãƒ«ãƒ€éšå±¤ãƒªã‚¹ãƒˆ\næ›´æ–°: {fh_info.get('last_updated', 'ä¸æ˜')}")
        else:
            st.warning("ãƒ•ã‚©ãƒ«ãƒ€éšå±¤ãƒªã‚¹ãƒˆ\næœªé…ç½®")

    # ãƒ•ã‚©ãƒ«ãƒ€éšå±¤ãƒªã‚¹ãƒˆã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½
    hierarchy_upload = st.file_uploader(
        "ãƒ•ã‚©ãƒ«ãƒ€éšå±¤ãƒªã‚¹ãƒˆã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆæ›´æ–°ï¼‰",
        type=['xlsx'],
        key="hierarchy_quick_upload",
        help="ãƒ•ã‚©ãƒ«ãƒ€éšå±¤ãƒªã‚¹ãƒˆ.xlsxã‚’ãƒ‰ãƒ©ãƒƒã‚°&ãƒ‰ãƒ­ãƒƒãƒ—ã—ã¦GitHubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"
    )
    if hierarchy_upload:
        if st.button("ğŸ“¤ ãƒ•ã‚©ãƒ«ãƒ€éšå±¤ãƒªã‚¹ãƒˆã‚’æ›´æ–°", type="secondary"):
            hierarchy_upload.seek(0)
            content = hierarchy_upload.read()
            result = upload_binary_to_github(
                content,
                GITHUB_FOLDER_HIERARCHY_PATH,
                f"Update folder_hierarchy.xlsx - {datetime.now().strftime('%Y-%m-%d %H:%M')}"
            )
            if result.get("success"):
                st.success("ãƒ•ã‚©ãƒ«ãƒ€éšå±¤ãƒªã‚¹ãƒˆã‚’æ›´æ–°ã—ã¾ã—ãŸ")
                st.session_state.github_folder_hierarchy = content
                st.rerun()
            else:
                st.error(f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {result.get('error')}")

    # CSVç”Ÿæˆãƒ»å–å¾—ã‚»ã‚¯ã‚·ãƒ§ãƒ³
    st.markdown("#### CSVãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œ")

    # æœ€æ–°ã®å®Ÿè¡Œå±¥æ­´ã‚’è¡¨ç¤ºï¼ˆæ—¥æœ¬æ™‚é–“ã«å¤‰æ›ï¼‰
    runs = get_workflow_runs("weekly-comic-lister.yml", limit=1)
    if runs:
        latest = runs[0]
        status_icon = "ğŸŸ¢" if latest["conclusion"] == "success" else "ğŸ”´" if latest["conclusion"] == "failure" else "ğŸŸ¡"
        # æ—¥æœ¬æ™‚é–“ã«å¤‰æ›ï¼ˆ+9æ™‚é–“ï¼‰
        from datetime import timedelta
        try:
            dt_utc = datetime.strptime(latest['created_at'], "%Y-%m-%d %H:%M")
            dt_jst = dt_utc + timedelta(hours=9)
            jst_str = dt_jst.strftime("%Y-%m-%d %H:%M")
        except:
            jst_str = latest['created_at']
        status_text = "å®Œäº†" if latest["conclusion"] == "success" else "å¤±æ•—" if latest["conclusion"] == "failure" else "å‡¦ç†ä¸­..."
        st.caption(f"å‰å›ç”Ÿæˆ: {jst_str} {status_icon} {status_text}")

    # ãƒœã‚¿ãƒ³ã‚’æ¨ªä¸¦ã³ã«é…ç½®ï¼ˆå·¦ã‚’ç›®ç«‹ã¤è‰²ã«ï¼‰
    btn_col1, btn_col2, _ = st.columns([3, 2, 3])

    with btn_col1:
        run_actions = st.button("ğŸ“Š is_list / comic_list ç”Ÿæˆ", type="primary", help="ä¸è¶³ã‚³ãƒŸãƒƒã‚¯ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•ç”Ÿæˆã—ã¾ã™", use_container_width=True)

    with btn_col2:
        fetch_files = st.button("ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", type="secondary", help="ç”Ÿæˆæ¸ˆã¿ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™", use_container_width=True)

    # GitHub Actions å®Ÿè¡Œå‡¦ç†
    if run_actions:
        with st.spinner("CSVãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆã‚’é–‹å§‹ä¸­..."):
            result = trigger_github_actions("weekly-comic-lister.yml")
        if result.get("success"):
            st.success("CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ç”Ÿæˆã‚’é–‹å§‹ã—ã¾ã—ãŸï¼ˆå®Œäº†ã¾ã§2ã€œ3åˆ†ãŠå¾…ã¡ãã ã•ã„ï¼‰")
        else:
            st.error(f"ç”Ÿæˆé–‹å§‹ã«å¤±æ•—ã—ã¾ã—ãŸ: {result.get('error')}")

    # GitHubã‹ã‚‰ä¸€æ‹¬å–å¾—å‡¦ç†
    if fetch_files:
        with st.spinner("GitHubã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—ä¸­..."):
            errors = []

            # is_list.csv
            result = download_from_github(GITHUB_IS_LIST_PATH)
            if result.get("success"):
                st.session_state.github_is_list = result["content"]
            else:
                errors.append(f"is_list.csv: {result.get('error')}")

            # comic_list.csv
            result = download_from_github(GITHUB_COMIC_LIST_PATH)
            if result.get("success"):
                st.session_state.github_comic_list = result["content"]
            else:
                errors.append(f"comic_list.csv: {result.get('error')}")

            # folder_hierarchy.xlsx
            result = download_from_github(GITHUB_FOLDER_HIERARCHY_PATH)
            if result.get("success"):
                st.session_state.github_folder_hierarchy = result["content"]
            else:
                errors.append(f"ãƒ•ã‚©ãƒ«ãƒ€éšå±¤ãƒªã‚¹ãƒˆ: {result.get('error')}")

        if errors:
            for err in errors:
                st.warning(err)
        else:
            st.success("å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã®å–å¾—ãŒå®Œäº†ã—ã¾ã—ãŸ")
        st.rerun()

    # å–å¾—æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã®è¡¨ç¤º
    status_cols = st.columns(3)
    with status_cols[0]:
        if st.session_state.github_is_list:
            st.info("âœ… is_list.csv å–å¾—æ¸ˆã¿")
    with status_cols[1]:
        if st.session_state.github_comic_list:
            st.info("âœ… comic_list.csv å–å¾—æ¸ˆã¿")
    with status_cols[2]:
        if st.session_state.github_folder_hierarchy:
            st.info("âœ… ãƒ•ã‚©ãƒ«ãƒ€éšå±¤ãƒªã‚¹ãƒˆ å–å¾—æ¸ˆã¿")

    st.divider()

    # ä½¿ç”¨ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ±ºå®šï¼ˆGitHubã‹ã‚‰å–å¾—ã—ãŸã‚‚ã®ï¼‰
    use_is_list = BytesIO(st.session_state.github_is_list) if st.session_state.github_is_list else None
    use_comic_list = BytesIO(st.session_state.github_comic_list) if st.session_state.github_comic_list else None
    use_hierarchy = BytesIO(st.session_state.github_folder_hierarchy) if st.session_state.github_folder_hierarchy else None

    # ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
    if use_is_list:
        st.markdown("### is_list.csv ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
        try:
            use_is_list.seek(0)
            # UTF-8ã‚’å…ˆã«è©¦ã—ã€å¤±æ•—ã—ãŸã‚‰cp932
            try:
                df_is_preview = pd.read_csv(use_is_list, encoding='utf-8', header=None)
            except:
                use_is_list.seek(0)
                df_is_preview = pd.read_csv(use_is_list, encoding='cp932', header=None)
            st.dataframe(df_is_preview.head(10), use_container_width=True, height=200)
            st.info(f"èª­ã¿è¾¼ã¿ä»¶æ•°: {len(df_is_preview)}è¡Œ")
        except Exception as e:
            st.error(f"CSVã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

    st.divider()

    st.markdown("### ç”»åƒå–å¾—")

    # å…¨ãƒ•ã‚¡ã‚¤ãƒ«ãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
    all_files_ready = use_is_list and use_comic_list and use_hierarchy

    if not all_files_ready:
        missing = []
        if not use_is_list:
            missing.append("is_list.csv")
        if not use_comic_list:
            missing.append("comic_list.csv")
        if not use_hierarchy:
            missing.append("ãƒ•ã‚©ãƒ«ãƒ€éšå±¤ãƒªã‚¹ãƒˆ.xlsx")
        st.info(f"ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¿…è¦ã§ã™: {', '.join(missing)}\n\nã€ŒGitHubã‹ã‚‰ä¸€æ‹¬å–å¾—ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã™ã‹ã€æ‰‹å‹•ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
    else:
        # ç”»åƒå–å¾—ãƒœã‚¿ãƒ³
        if st.button("ğŸ–¼ï¸ ç”»åƒå–å¾—é–‹å§‹", type="primary"):
            try:
                # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ï¼ˆUTF-8ã‚’å…ˆã«è©¦ã—ã€å¤±æ•—ã—ãŸã‚‰cp932ï¼‰
                use_is_list.seek(0)
                use_comic_list.seek(0)
                use_hierarchy.seek(0)

                with st.spinner("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
                    # is_list.csv
                    try:
                        df_is = pd.read_csv(use_is_list, encoding='utf-8', header=None)
                    except:
                        use_is_list.seek(0)
                        df_is = pd.read_csv(use_is_list, encoding='cp932', header=None)

                    # comic_list.csv
                    try:
                        df_cl = pd.read_csv(use_comic_list, encoding='utf-8', header=None)
                    except:
                        use_comic_list.seek(0)
                        df_cl = pd.read_csv(use_comic_list, encoding='cp932', header=None)

                    df_hierarchy = pd.read_excel(use_hierarchy, sheet_name="ãƒ•ã‚©ãƒ«ãƒ€éšå±¤ãƒªã‚¹ãƒˆ", header=None)

                st.success(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: IS={len(df_is)}è¡Œ, CL={len(df_cl)}è¡Œ, éšå±¤={len(df_hierarchy)}è¡Œ")

                # ãƒ‡ãƒ¼ã‚¿çµ±åˆ
                with st.spinner("ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆä¸­..."):
                    merged_df = merge_csv_data(df_is.copy(), df_cl)
                    result_data = extract_first_volumes(merged_df)
                    result_data = add_folder_hierarchy_info(result_data, df_hierarchy)

                # JANã‚³ãƒ¼ãƒ‰ã®çŠ¶æ…‹ã‚’ç¢ºèª
                jan_count = sum(1 for d in result_data if d.get('first_jan') and normalize_jan_code(d.get('first_jan', '')))
                no_jan_count = len(result_data) - jan_count
                st.success(f"ãƒ‡ãƒ¼ã‚¿çµ±åˆå®Œäº†: {len(result_data)}ä»¶ï¼ˆJANã‚ã‚Š: {jan_count}ä»¶, JANãªã—: {no_jan_count}ä»¶ï¼‰")

                # JANã‚³ãƒ¼ãƒ‰ãŒãªã„å ´åˆã¯è©³ç´°ã‚’è¡¨ç¤º
                if no_jan_count > 0:
                    no_jan_items = [d for d in result_data if not normalize_jan_code(d.get('first_jan', ''))]
                    with st.expander(f"âš ï¸ JANã‚³ãƒ¼ãƒ‰ãªã—: {no_jan_count}ä»¶ï¼ˆè©³ç´°ï¼‰"):
                        for item in no_jan_items[:10]:  # æœ€å¤§10ä»¶è¡¨ç¤º
                            st.write(f"- {item.get('comic_no', '?')}: {item.get('title', '?')} (first_jan='{item.get('first_jan', '')}')")

                # ç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                st.markdown("### ç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")

                # Gemini AIçŠ¶æ…‹ã‚’è¡¨ç¤º
                if GEMINI_API_KEY:
                    st.info("ğŸ¤– Gemini AI ã‚»ãƒ«ãƒ•ãƒ’ãƒ¼ãƒªãƒ³ã‚°: æœ‰åŠ¹ï¼ˆAPIã‚­ãƒ¼è¨­å®šæ¸ˆã¿ï¼‰")
                else:
                    st.warning("ğŸ¤– Gemini AI ã‚»ãƒ«ãƒ•ãƒ’ãƒ¼ãƒªãƒ³ã‚°: ç„¡åŠ¹ï¼ˆGEMINI_API_KEYæœªè¨­å®šï¼‰")

                progress_bar = st.progress(0)
                status_text = st.empty()

                session = requests.Session()
                downloaded_images = []
                stats = {'total': len(result_data), 'success': 0, 'bookoff': 0, 'amazon': 0, 'rakuten': 0, 'gemini_ai': 0, 'failed': 0}

                random = get_random()
                for i, data in enumerate(result_data):
                    jan_code = normalize_jan_code(data['first_jan'])
                    comic_no = data['comic_no']

                    progress_bar.progress((i + 1) / len(result_data))
                    status_text.text(f"å‡¦ç†ä¸­: {comic_no} ({i + 1}/{len(result_data)}) JAN: {jan_code or '(ãªã—)'}")

                    if not jan_code:
                        stats['failed'] += 1
                        stats['failed_no_jan'] = stats.get('failed_no_jan', 0) + 1
                        continue

                    # 1. ãƒ–ãƒƒã‚¯ã‚ªãƒ•ã§æ¤œç´¢
                    image_url = get_bookoff_image(jan_code, session)
                    source = 'bookoff'

                    # 2. Amazonã§æ¤œç´¢
                    if not image_url:
                        time.sleep(random.uniform(0.5, 1.0))
                        image_url = get_amazon_image(jan_code, session)
                        source = 'amazon'

                    # 3. æ¥½å¤©ãƒ–ãƒƒã‚¯ã‚¹ã§æ¤œç´¢ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
                    if not image_url:
                        time.sleep(random.uniform(0.3, 0.6))
                        image_url = get_rakuten_image(jan_code, session)
                        source = 'rakuten'

                    # 4. Gemini AIã§ã‚»ãƒ«ãƒ•ãƒ’ãƒ¼ãƒªãƒ³ã‚°ï¼ˆå…¨ã¦å¤±æ•—ã—ãŸå ´åˆï¼‰
                    # ãƒ‡ãƒãƒƒã‚°: AIä¿®å¾©æ¡ä»¶ã‚’è¨˜éŒ²
                    ai_condition = f"image_url={bool(image_url)}, GEMINI_API_KEY={bool(GEMINI_API_KEY)}"
                    if not image_url and GEMINI_API_KEY:
                        time.sleep(random.uniform(0.5, 1.0))
                        status_text.text(f"å‡¦ç†ä¸­: {comic_no} ({i + 1}/{len(result_data)}) - AIè§£æä¸­...")
                        stats['gemini_tried'] = stats.get('gemini_tried', 0) + 1
                        # Amazonã‚’å†è©¦è¡Œï¼ˆAIã§HTMLè§£æï¼‰
                        ai_result = get_image_with_gemini_ai(jan_code, session, "amazon")
                        if ai_result:
                            image_url = ai_result
                            source = 'gemini_ai'
                    elif not image_url and not GEMINI_API_KEY:
                        # GEMINI_API_KEYãŒãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—
                        stats['ai_skipped_no_key'] = stats.get('ai_skipped_no_key', 0) + 1

                    if image_url:
                        image_data = download_image(image_url, session)
                        if image_data:
                            downloaded_images.append({
                                'filename': f"{comic_no}.jpg",
                                'data': image_data,
                                'comic_no': comic_no,
                                'jan': jan_code,
                                'title': data['title']
                            })
                            stats['success'] += 1
                            stats[source] += 1
                        else:
                            stats['failed'] += 1
                            stats['failed_download'] = stats.get('failed_download', 0) + 1
                            # ãƒ‡ãƒãƒƒã‚°: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—ã®URLã‚’è¨˜éŒ²
                            stats['debug_failed_urls'] = stats.get('debug_failed_urls', [])
                            stats['debug_failed_urls'].append({'comic_no': comic_no, 'url': image_url[:100]})
                    else:
                        stats['failed'] += 1
                        stats['failed_not_found'] = stats.get('failed_not_found', 0) + 1

                    time.sleep(0.3)

                progress_bar.empty()
                status_text.empty()

                # çµæœã‚’session_stateã«ä¿å­˜
                st.session_state.image_download_result = {
                    'stats': stats,
                    'downloaded_images': downloaded_images,
                    'result_data': result_data
                }
                st.rerun()

            except Exception as e:
                st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                import traceback
                st.code(traceback.format_exc())

    # çµæœè¡¨ç¤ºï¼ˆsession_stateã‹ã‚‰ï¼‰
    if st.session_state.image_download_result:
        result = st.session_state.image_download_result
        stats = result['stats']
        downloaded_images = result['downloaded_images']
        result_data = result['result_data']

        # çµæœã‚µãƒãƒªãƒ¼
        st.markdown("### çµæœ")
        col1, col2, col3, col4, col5, col6 = st.columns(6)
        col1.metric("ç·æ•°", stats['total'])
        col2.metric("æˆåŠŸ", stats['success'])
        col3.metric("ãƒ–ãƒƒã‚¯ã‚ªãƒ•", stats['bookoff'])
        col4.metric("Amazon", stats['amazon'])
        col5.metric("æ¥½å¤©", stats.get('rakuten', 0))
        col6.metric("AIä¿®å¾©", stats.get('gemini_ai', 0))

        # Gemini AIè©¦è¡Œå›æ•°ã‚’è¡¨ç¤º
        gemini_tried = stats.get('gemini_tried', 0)
        failed_no_jan = stats.get('failed_no_jan', 0)
        failed_not_found = stats.get('failed_not_found', 0)
        failed_download = stats.get('failed_download', 0)

        if stats['failed'] > 0:
            # å¤±æ•—ã®è©³ç´°
            failed_details = []
            if failed_no_jan > 0:
                failed_details.append(f"JANã‚³ãƒ¼ãƒ‰ãªã—: {failed_no_jan}ä»¶")
            if failed_not_found > 0:
                failed_details.append(f"ç”»åƒè¦‹ã¤ã‹ã‚‰ãš: {failed_not_found}ä»¶")
            if failed_download > 0:
                failed_details.append(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {failed_download}ä»¶")

            # è©³ç´°ãŒãªã„å ´åˆã¯å¤ã„çµæœã®å¯èƒ½æ€§
            if not failed_details:
                failed_details.append("è©³ç´°ä¸æ˜ï¼ˆå¤ã„çµæœï¼Ÿâ†’ã‚¯ãƒªã‚¢ã—ã¦å†å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼‰")

            st.warning(f"å–å¾—ã§ããªã‹ã£ãŸç”»åƒ: {stats['failed']}ä»¶ ({', '.join(failed_details)})")

            # AIä¿®å¾©ã®çŠ¶æ…‹
            ai_skipped_no_key = stats.get('ai_skipped_no_key', 0)
            if GEMINI_API_KEY:
                if gemini_tried > 0:
                    st.info(f"ğŸ¤– Gemini AIè©¦è¡Œ: {gemini_tried}å› â†’ æˆåŠŸ: {stats.get('gemini_ai', 0)}å›")
                elif failed_no_jan == stats['failed']:
                    st.info("ğŸ¤– AIä¿®å¾©: JANã‚³ãƒ¼ãƒ‰ãŒãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ï¼ˆAIä¿®å¾©ã«ã‚‚JANã‚³ãƒ¼ãƒ‰ãŒå¿…è¦ã§ã™ï¼‰")
                elif ai_skipped_no_key > 0:
                    st.warning(f"ğŸ¤– AIä¿®å¾©: APIã‚­ãƒ¼ãŒå®Ÿè¡Œæ™‚ã«ç©ºã ã£ãŸï¼ˆ{ai_skipped_no_key}ä»¶ã‚¹ã‚­ãƒƒãƒ—ï¼‰")
                elif failed_not_found > 0:
                    st.warning("ğŸ¤– AIä¿®å¾©ãŒè©¦è¡Œã•ã‚Œã¾ã›ã‚“ã§ã—ãŸï¼ˆè¦èª¿æŸ»ï¼šç”»åƒãŒè¦‹ã¤ã‹ã‚‰ãªã„ã®ã«AIãŒç™ºå‹•ã—ã¦ã„ãªã„ï¼‰")
            else:
                st.warning("ğŸ¤– Gemini APIã‚­ãƒ¼ãŒæœªè¨­å®šã®ãŸã‚ã€AIä¿®å¾©ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸ")

            # ãƒ‡ãƒãƒƒã‚°æƒ…å ±
            with st.expander("ğŸ”§ ãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼ˆè©³ç´°ï¼‰"):
                st.write(f"**statså…¨ä½“:** {stats}")
                st.write(f"**GEMINI_API_KEYè¨­å®š:** {'ã‚ã‚Š' if GEMINI_API_KEY else 'ãªã—'}")
                if stats.get('debug_failed_urls'):
                    st.write("**ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—URL:**")
                    for item in stats['debug_failed_urls'][:5]:
                        st.write(f"  - {item['comic_no']}: {item['url']}")

        # ZIPãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        if downloaded_images:
            st.divider()
            st.markdown("### ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")

            # ZIPä½œæˆ
            zipfile = get_zipfile()
            zip_buffer = BytesIO()
            with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zf:
                for img in downloaded_images:
                    zf.writestr(img['filename'], img['data'])
            zip_buffer.seek(0)

            # æŒ¯ã‚Šåˆ†ã‘ãƒãƒƒãƒ—Excelä½œæˆ
            excel_data = []
            for i, data in enumerate(result_data, 1):
                excel_data.append({
                    'é€£ç•ª': i,
                    'ã‚³ãƒŸãƒƒã‚¯No': data['comic_no'],
                    '1å·»JAN': data['first_jan'],
                    'ã‚¿ã‚¤ãƒˆãƒ«': data['title'],
                    'ã‚¸ãƒ£ãƒ³ãƒ«': data['genre'],
                    'å‡ºç‰ˆç¤¾': data['publisher'],
                    'è‘—è€…': data['author'],
                    'ã‚·ãƒªãƒ¼ã‚º': data['series'],
                    'ãƒ¡ã‚¤ãƒ³ãƒ•ã‚©ãƒ«ãƒ€': data.get('main_folder', ''),
                    'ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€': data.get('sub_folder', '')
                })

            df_excel = pd.DataFrame(excel_data)
            excel_buffer = BytesIO()
            with pd.ExcelWriter(excel_buffer, engine='openpyxl') as writer:
                df_excel.to_excel(writer, index=False, sheet_name='æŒ¯ã‚Šåˆ†ã‘ãƒãƒƒãƒ—')
                style_excel(writer.sheets['æŒ¯ã‚Šåˆ†ã‘ãƒãƒƒãƒ—'], num_columns=10)
            excel_buffer.seek(0)

            # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ã‚’æ¨ªä¸¦ã³ã«
            dl_col1, dl_col2, dl_col3 = st.columns([2, 2, 1])
            with dl_col1:
                st.download_button(
                    label=f"ğŸ“¥ ç”»åƒZIP ({len(downloaded_images)}ä»¶)",
                    data=zip_buffer,
                    file_name="comic_images.zip",
                    mime="application/zip",
                    key="zip_download"
                )
            with dl_col2:
                st.download_button(
                    label="ğŸ“¥ æŒ¯ã‚Šåˆ†ã‘ãƒãƒƒãƒ—Excel",
                    data=excel_buffer,
                    file_name="æŒ¯ã‚Šåˆ†ã‘ãƒãƒƒãƒ—.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    key="excel_download"
                )
            with dl_col3:
                if st.button("ğŸ—‘ï¸ ã‚¯ãƒªã‚¢"):
                    st.session_state.image_download_result = None
                    st.rerun()
