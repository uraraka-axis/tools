#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¥½å¤©å¸‚å ´ ã‚«ãƒ†ã‚´ãƒªæŠ½å‡ºãƒ„ãƒ¼ãƒ« - Streamlitç‰ˆ
Webãƒ–ãƒ©ã‚¦ã‚¶ã‹ã‚‰ä½¿ç”¨å¯èƒ½ãªãƒãƒ¼ã‚¸ãƒ§ãƒ³
"""

import streamlit as st
import requests
from bs4 import BeautifulSoup
from openpyxl import Workbook
from openpyxl.styles import Font, PatternFill, Alignment, Border, Side
from openpyxl.utils import get_column_letter
from typing import List, Dict
from dataclasses import dataclass, field
from datetime import datetime
from collections import defaultdict
import random
import time
import re
import io


@dataclass
class Category:
    """ã‚«ãƒ†ã‚´ãƒªæƒ…å ±ã‚’ä¿æŒã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    name: str
    category_id: str
    url: str
    count: int
    level: int
    parent_path: List[str] = field(default_factory=list)


class RakutenCategoryScraper:
    """æ¥½å¤©å¸‚å ´ã‚«ãƒ†ã‚´ãƒªã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ (requestsç‰ˆ)"""

    BASE_URL = "https://www.rakuten.co.jp"

    def __init__(self):
        self.session = requests.Session()
        # ãƒ–ãƒ©ã‚¦ã‚¶ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¨­å®šï¼ˆbotæ¤œå‡ºå›é¿ï¼‰
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Cache-Control': 'max-age=0',
            'Sec-Ch-Ua': '"Not A(Brand";v="99", "Google Chrome";v="121", "Chromium";v="121"',
            'Sec-Ch-Ua-Mobile': '?0',
            'Sec-Ch-Ua-Platform': '"Windows"',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Upgrade-Insecure-Requests': '1',
        })
        self.stop_flag = False
        self.categories: List[Category] = []
        self.visited_ids = set()
        self.root_category_name = ""
        self.root_category_id = ""
        self.total_requests = 0
        self.log_messages = []

        # å¾…æ©Ÿæ™‚é–“è¨­å®š
        self.min_delay = 1.5
        self.max_delay = 4.0

    def log(self, message: str):
        """ãƒ­ã‚°å‡ºåŠ›"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        log_message = f"[{timestamp}] {message}"
        self.log_messages.append(log_message)

    def stop(self):
        """å‡¦ç†ã‚’åœæ­¢"""
        self.stop_flag = True

    def random_delay(self):
        """ãƒ©ãƒ³ãƒ€ãƒ ãªå¾…æ©Ÿæ™‚é–“"""
        delay = random.uniform(self.min_delay, self.max_delay)
        if random.random() < 0.1:
            delay += random.uniform(1.0, 3.0)
        time.sleep(delay)

    def extract_category_id_from_url(self, url: str) -> str:
        """URLã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªIDã‚’æŠ½å‡º"""
        match = re.search(r'/category/(\d+)/?', url)
        if match:
            return match.group(1)
        return ""

    def fetch_page(self, url: str):
        """ãƒšãƒ¼ã‚¸ã‚’å–å¾—"""
        try:
            self.total_requests += 1

            if self.total_requests > 1:
                self.random_delay()

            response = self.session.get(url, timeout=30)
            response.raise_for_status()

            return BeautifulSoup(response.text, 'html.parser')

        except Exception as e:
            self.log(f"  âš ï¸ ãƒšãƒ¼ã‚¸å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def get_root_category_name(self, soup: BeautifulSoup) -> str:
        """ãƒ«ãƒ¼ãƒˆã‚«ãƒ†ã‚´ãƒªåã‚’å–å¾—"""
        # ãƒ‘ãƒ³ããšãƒªã‚¹ãƒˆã‹ã‚‰å–å¾—
        breadcrumb = soup.find('div', class_='dui-container breadcrumb')
        if breadcrumb:
            items = breadcrumb.find_all('a', class_='item')
            if items:
                return items[-1].get_text(strip=True)

        # -activeã‚¯ãƒ©ã‚¹ã‚’æŒã¤spanã‹ã‚‰å–å¾—
        active = soup.find('span', class_=re.compile(r'-active'))
        if active:
            return active.get_text(strip=True)

        # h1ã‹ã‚‰å–å¾—
        h1 = soup.find('h1')
        if h1:
            return h1.get_text(strip=True)

        return "ã‚«ãƒ†ã‚´ãƒª"

    def get_subcategories_from_page(self, soup: BeautifulSoup, current_category_id: str) -> List[Dict]:
        """ãƒšãƒ¼ã‚¸ã‹ã‚‰ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªã‚’æŠ½å‡º"""
        subcategories = []

        # ã‚¸ãƒ£ãƒ³ãƒ«ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’æ¢ã™
        genre_filter = None

        # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰æ¢ã™
        sidebar = soup.find('div', class_=re.compile(r'sidebar|side-menu'))
        if sidebar:
            genre_section = sidebar.find('div', class_=re.compile(r'genre|category'))
            if genre_section:
                genre_filter = genre_section

        if not genre_filter:
            genre_filter = soup.find('div', class_=re.compile(r'genrefilter|genre_filter|genre-list'))

        if not genre_filter:
            genre_filter = soup.find('div', class_='dui-filter-menu')

        if not genre_filter:
            self.log("    âš ï¸ ã‚¸ãƒ£ãƒ³ãƒ«ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            # HTMLãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return self._extract_categories_from_html(soup, current_category_id)

        # ç¾åœ¨ã®ã‚«ãƒ†ã‚´ãƒªï¼ˆ-activeã‚¯ãƒ©ã‚¹ã‚’æŒã¤spanï¼‰ã‚’æ¢ã™
        active_element = genre_filter.find(['span', 'div'], class_=re.compile(r'-active'))

        if not active_element:
            self.log("    âš ï¸ ç¾åœ¨ã®ã‚«ãƒ†ã‚´ãƒªè¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return self._extract_categories_from_html(soup, current_category_id)

        # ç¾åœ¨ã®ã‚«ãƒ†ã‚´ãƒªã®æ¬¡ã®å…„å¼Ÿè¦ç´ ï¼ˆdiv.itemï¼‰ã‚’å–å¾—
        child_container = active_element.find_next_sibling('div', class_='item')

        if not child_container:
            self.log("    â„¹ï¸ ã“ã®ã‚«ãƒ†ã‚´ãƒªã«ã¯å­ã‚«ãƒ†ã‚´ãƒªãŒã‚ã‚Šã¾ã›ã‚“")
            return []

        # å­ã‚³ãƒ³ãƒ†ãƒŠå†…ã®dui-listã‚’æ¢ã™
        child_list = child_container.find('div', class_='dui-list')

        if not child_list:
            self.log("    â„¹ï¸ ã“ã®ã‚«ãƒ†ã‚´ãƒªã«ã¯å­ã‚«ãƒ†ã‚´ãƒªãŒã‚ã‚Šã¾ã›ã‚“")
            return []

        # å­ãƒªã‚¹ãƒˆå†…ã®ãƒªãƒ³ã‚¯ã‚’å–å¾—
        category_links = child_list.find_all('a', href=re.compile(r'/category/\d+/?'))

        self.log(f"    æ¤œå‡ºãƒªãƒ³ã‚¯æ•°: {len(category_links)}, ç¾åœ¨ID: {current_category_id}")

        for link in category_links:
            href = link.get('href', '')

            category_id = self.extract_category_id_from_url(href)
            if not category_id:
                continue

            if category_id == current_category_id:
                continue

            if category_id in self.visited_ids:
                continue

            # -activeã‚¯ãƒ©ã‚¹ã‚’æŒã¤ãƒªãƒ³ã‚¯ã¯é™¤å¤–
            link_classes = link.get('class', [])
            if link_classes and '-active' in ' '.join(link_classes):
                continue

            # åå‰ã‚’å–å¾—
            name = link.get('title', '')
            if not name:
                name_elem = link.find('div', class_='_ellipsis')
                if name_elem:
                    name = name_elem.get_text(strip=True)
                else:
                    name = link.get_text(strip=True)

            # ä»¶æ•°ã‚’é™¤å»
            name = re.sub(r'\s*[\(ï¼ˆ]\s*[\d,]+\s*[ä»¶ç‚¹]\s*[\)ï¼‰]\s*$', '', name)
            name = name.strip()

            if not name:
                continue

            # URLã‚’æ­£è¦åŒ–
            if href.startswith('//'):
                full_url = 'https:' + href
            elif href.startswith('/'):
                full_url = self.BASE_URL + href
            else:
                full_url = href

            subcategories.append({
                'name': name,
                'url': full_url,
                'category_id': category_id,
                'count': 0
            })

        # é‡è¤‡é™¤å»
        seen = set()
        unique = []
        for cat in subcategories:
            if cat['category_id'] not in seen and cat['name']:
                seen.add(cat['category_id'])
                unique.append(cat)

        return unique

    def _extract_categories_from_html(self, soup: BeautifulSoup, current_category_id: str) -> List[Dict]:
        """HTMLã‹ã‚‰ç›´æ¥ã‚«ãƒ†ã‚´ãƒªãƒªãƒ³ã‚¯ã‚’æŠ½å‡ºï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
        subcategories = []
        seen_ids = set()

        # ã‚«ãƒ†ã‚´ãƒªãƒªãƒ³ã‚¯ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
        category_pattern = re.compile(r'/category/(\d+)/?')

        # ã™ã¹ã¦ã®ã‚«ãƒ†ã‚´ãƒªãƒªãƒ³ã‚¯ã‚’æ¢ã™
        for link in soup.find_all('a', href=category_pattern):
            href = link.get('href', '')
            match = category_pattern.search(href)
            if not match:
                continue

            category_id = match.group(1)

            # è‡ªåˆ†è‡ªèº«ã¯é™¤å¤–
            if category_id == current_category_id:
                continue

            # é‡è¤‡é™¤å»
            if category_id in seen_ids:
                continue
            seen_ids.add(category_id)

            # åå‰ã‚’å–å¾—
            name = link.get('title', '') or link.get_text(strip=True)

            # ä»¶æ•°ã‚’é™¤å»
            name = re.sub(r'\s*[\(ï¼ˆ]\s*[\d,]+\s*[ä»¶ç‚¹]\s*[\)ï¼‰]\s*$', '', name)
            name = re.sub(r'[\d,]+ä»¶$', '', name).strip()

            if not name:
                continue

            # ç„¡åŠ¹ãªåå‰ã‚’ã‚¹ã‚­ãƒƒãƒ—
            if name in ['ã‚‚ã£ã¨è¦‹ã‚‹', 'ã™ã¹ã¦è¦‹ã‚‹', 'è©³ç´°ã‚’è¦‹ã‚‹', 'é–‰ã˜ã‚‹']:
                continue

            # URLã‚’æ­£è¦åŒ–
            if href.startswith('//'):
                full_url = 'https:' + href
            elif href.startswith('/'):
                full_url = self.BASE_URL + href
            else:
                full_url = href

            subcategories.append({
                'name': name,
                'url': full_url,
                'category_id': category_id,
                'count': 0
            })

        self.log(f"    [DEBUG] HTMLã‹ã‚‰ {len(subcategories)} ä»¶ã®ã‚«ãƒ†ã‚´ãƒªãƒªãƒ³ã‚¯ã‚’æ¤œå‡º")
        return subcategories

    def scrape_categories_recursive(
        self,
        url: str,
        level: int = 0,
        parent_path: List[str] = None,
        max_depth: int = 5,
        progress_callback=None
    ):
        """ã‚«ãƒ†ã‚´ãƒªã‚’å†å¸°çš„ã«å–å¾—"""
        if parent_path is None:
            parent_path = []

        if level > max_depth or self.stop_flag:
            return

        indent = "  " * level
        self.log(f"{indent}ğŸ“‚ å–å¾—ä¸­: {url}")

        soup = self.fetch_page(url)
        if not soup:
            return

        current_id = self.extract_category_id_from_url(url)
        self.visited_ids.add(current_id)

        is_root = (level == 0)
        if is_root:
            self.root_category_name = self.get_root_category_name(soup)
            self.root_category_id = current_id
            self.log(f"ğŸ“Œ ãƒ«ãƒ¼ãƒˆã‚«ãƒ†ã‚´ãƒª: {self.root_category_name} (ID: {current_id})")

        subcategories = self.get_subcategories_from_page(soup, current_id)

        self.log(f"{indent}  â†’ {len(subcategories)}ä»¶ã®ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªã‚’ç™ºè¦‹")

        for subcat in subcategories:
            if self.stop_flag:
                break

            if subcat['category_id'] in self.visited_ids:
                continue

            self.visited_ids.add(subcat['category_id'])

            cat = Category(
                name=subcat['name'],
                category_id=subcat['category_id'],
                url=subcat['url'],
                count=subcat['count'],
                level=level + 1,
                parent_path=parent_path.copy()
            )
            self.categories.append(cat)

            self.log(f"{indent}  âœ“ {subcat['name']} [ID: {subcat['category_id']}]")

            # é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if progress_callback:
                progress_callback(len(self.categories), parent_path + [subcat['name']])

            # å†å¸°
            if level + 1 < max_depth:
                new_parent_path = parent_path + [subcat['name']]
                self.scrape_categories_recursive(
                    subcat['url'],
                    level + 1,
                    new_parent_path,
                    max_depth,
                    progress_callback
                )

    def scrape(self, start_url: str, max_depth: int = 5, progress_callback=None) -> List[Category]:
        """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹"""
        self.stop_flag = False
        self.categories = []
        self.visited_ids = set()
        self.total_requests = 0
        self.log_messages = []

        self.log("=" * 50)
        self.log("ğŸ›’ æ¥½å¤©å¸‚å ´ ã‚«ãƒ†ã‚´ãƒªæŠ½å‡ºé–‹å§‹")
        self.log("=" * 50)
        self.log(f"ğŸ”— URL: {start_url}")
        self.log(f"ğŸ“Š æœ€å¤§å–å¾—éšå±¤: {max_depth}")
        self.log("")

        self.scrape_categories_recursive(start_url, max_depth=max_depth, progress_callback=progress_callback)

        if not self.stop_flag:
            self.log("")
            self.log(f"âœ… åˆè¨ˆ {len(self.categories)} ã‚«ãƒ†ã‚´ãƒªã‚’å–å¾—ã—ã¾ã—ãŸ")
            self.log(f"ğŸ“¡ ç·ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°: {self.total_requests}")

        return self.categories

    def export_to_excel(self) -> bytes:
        """Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚¤ãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ ã§å‡ºåŠ›"""
        if not self.categories:
            return None

        wb = Workbook()
        ws = wb.active
        ws.title = "ã‚«ãƒ†ã‚´ãƒªä¸€è¦§"

        base_font = Font(name="Meiryo UI", size=10)
        header_font = Font(name="Meiryo UI", bold=True, color="FFFFFF", size=11)
        header_fill = PatternFill("solid", fgColor="BF0000")
        header_alignment = Alignment(horizontal="center", vertical="center", wrap_text=True)
        title_font = Font(name="Meiryo UI", bold=True, size=14, color="BF0000")

        thin_border = Border(
            left=Side(style='thin', color='595959'),
            right=Side(style='thin', color='595959'),
            top=Side(style='thin', color='595959'),
            bottom=Side(style='thin', color='595959')
        )

        max_level = max((cat.level for cat in self.categories), default=1)

        level_counts = defaultdict(int)
        for cat in self.categories:
            level_counts[cat.level] += 1

        summary_level_col = 2 + max_level + 4
        summary_count_col = 2 + max_level + 5

        title_col_end = get_column_letter(summary_count_col)
        ws.merge_cells(f'B1:{title_col_end}1')
        ws['B1'] = f"ã€æ¥½å¤©å¸‚å ´ã€‘{self.root_category_name}ã®ã‚¸ãƒ£ãƒ³ãƒ«ä¸€è¦§"
        ws['B1'].font = title_font
        ws['B1'].alignment = Alignment(horizontal="center", vertical="center")
        ws.row_dimensions[1].height = 28
        ws.row_dimensions[2].height = 8

        headers = ["#", "ã‚¸ãƒ£ãƒ³ãƒ«1"]
        for i in range(max_level):
            headers.append(f"ã‚¸ãƒ£ãƒ³ãƒ«{i + 2}")
        headers.append("ã‚«ãƒ†ã‚´ãƒªID")
        headers.append("ãƒšãƒ¼ã‚¸URL")

        for col, header in enumerate(headers, 1):
            cell = ws.cell(row=3, column=col, value=header)
            cell.font = header_font
            cell.fill = header_fill
            cell.alignment = header_alignment
            cell.border = thin_border

        ws.row_dimensions[3].height = 24

        summary_level_header = ws.cell(row=3, column=summary_level_col, value="ãƒ¬ãƒ™ãƒ«")
        summary_level_header.font = header_font
        summary_level_header.fill = header_fill
        summary_level_header.alignment = header_alignment
        summary_level_header.border = thin_border

        summary_count_header = ws.cell(row=3, column=summary_count_col, value="ã‚«ãƒ†ã‚´ãƒªæ•°")
        summary_count_header.font = header_font
        summary_count_header.fill = header_fill
        summary_count_header.alignment = header_alignment
        summary_count_header.border = thin_border

        prev_values = [""] * (max_level + 1)

        for idx, cat in enumerate(self.categories, 1):
            row = idx + 3

            current_values = [self.root_category_name] + [""] * max_level

            for i, parent_name in enumerate(cat.parent_path):
                if i < max_level:
                    current_values[i + 1] = parent_name

            if cat.level <= max_level:
                current_values[cat.level] = cat.name

            cell = ws.cell(row=row, column=1, value=idx)
            cell.border = thin_border
            cell.font = base_font

            for col, value in enumerate(current_values, 2):
                cell = ws.cell(row=row, column=col)

                show_value = value
                if idx > 1 and col - 2 < len(prev_values):
                    if value == prev_values[col - 2]:
                        has_change = any(
                            current_values[j] != prev_values[j]
                            for j in range(col - 1, len(current_values))
                            if j < len(prev_values)
                        )
                        if not has_change:
                            show_value = ""

                cell.value = show_value
                cell.border = thin_border
                cell.font = base_font

            id_col = 2 + max_level + 1
            id_cell = ws.cell(row=row, column=id_col, value=cat.category_id)
            id_cell.border = thin_border
            id_cell.font = base_font

            url_col = 2 + max_level + 2
            url_cell = ws.cell(row=row, column=url_col, value=cat.url)
            url_cell.hyperlink = cat.url
            url_cell.style = "Hyperlink"
            url_cell.border = thin_border
            url_cell.font = Font(name="Meiryo UI", size=10, color="0563C1", underline="single")

            prev_values = current_values.copy()

        summary_row = 4

        level_cell = ws.cell(row=summary_row, column=summary_level_col, value="ã‚¸ãƒ£ãƒ³ãƒ«1")
        level_cell.border = thin_border
        level_cell.font = base_font

        count_cell = ws.cell(row=summary_row, column=summary_count_col, value=1)
        count_cell.border = thin_border
        count_cell.font = base_font
        count_cell.alignment = Alignment(horizontal="right")

        summary_row += 1

        for level in sorted(level_counts.keys()):
            level_cell = ws.cell(row=summary_row, column=summary_level_col, value=f"ã‚¸ãƒ£ãƒ³ãƒ«{level + 1}")
            level_cell.border = thin_border
            level_cell.font = base_font

            count_cell = ws.cell(row=summary_row, column=summary_count_col, value=level_counts[level])
            count_cell.border = thin_border
            count_cell.font = base_font
            count_cell.alignment = Alignment(horizontal="right")

            summary_row += 1

        total_cell = ws.cell(row=summary_row, column=summary_level_col, value="åˆè¨ˆ")
        total_cell.border = thin_border
        total_cell.font = Font(name="Meiryo UI", size=10, bold=True)

        total_count_cell = ws.cell(row=summary_row, column=summary_count_col, value=len(self.categories) + 1)
        total_count_cell.border = thin_border
        total_count_cell.font = Font(name="Meiryo UI", size=10, bold=True)
        total_count_cell.alignment = Alignment(horizontal="right")

        ws.column_dimensions['A'].width = 6
        ws.column_dimensions['B'].width = 18
        for i in range(max_level):
            col_letter = get_column_letter(3 + i)
            ws.column_dimensions[col_letter].width = 22
        ws.column_dimensions[get_column_letter(3 + max_level)].width = 12
        # URLåˆ—ã®å¹…ã‚’æœ€é•·URLã«åˆã‚ã›ã¦èª¿æ•´
        max_url_length = max((len(cat.url) for cat in self.categories), default=50)
        url_col_width = min(max(max_url_length * 1.1, 50), 120)
        ws.column_dimensions[get_column_letter(4 + max_level)].width = url_col_width
        ws.column_dimensions[get_column_letter(5 + max_level)].width = 3
        ws.column_dimensions[get_column_letter(summary_level_col)].width = 12
        ws.column_dimensions[get_column_letter(summary_count_col)].width = 12

        ws.freeze_panes = 'A4'

        # ãƒã‚¤ãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ ã¨ã—ã¦å‡ºåŠ›
        output = io.BytesIO()
        wb.save(output)
        output.seek(0)
        return output.getvalue()


def check_password():
    """ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰èªè¨¼"""

    # secretsã«ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
    if "password" not in st.secrets:
        # ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰æœªè¨­å®šã®å ´åˆã¯ãã®ã¾ã¾ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½
        return True

    correct_password = st.secrets["password"]

    if "authenticated" not in st.session_state:
        st.session_state.authenticated = False

    if st.session_state.authenticated:
        return True

    st.markdown("""
        <style>
        .login-container {
            max-width: 400px;
            margin: 100px auto;
            padding: 40px;
            background: white;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        </style>
    """, unsafe_allow_html=True)

    st.title("ğŸ” ãƒ­ã‚°ã‚¤ãƒ³")
    st.write("ã“ã®ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒå¿…è¦ã§ã™ã€‚")

    password = st.text_input("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰", type="password", key="password_input")

    if st.button("ãƒ­ã‚°ã‚¤ãƒ³", type="primary"):
        if password == correct_password:
            st.session_state.authenticated = True
            st.rerun()
        else:
            st.error("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒé•ã„ã¾ã™")

    return False


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""

    st.set_page_config(
        page_title="æ¥½å¤©å¸‚å ´ ã‚«ãƒ†ã‚´ãƒªæŠ½å‡ºãƒ„ãƒ¼ãƒ«",
        page_icon="ğŸ›’",
        layout="wide"
    )

    # ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰èªè¨¼
    if not check_password():
        return

    # ã‚«ã‚¹ã‚¿ãƒ CSS
    st.markdown("""
        <style>
        .main-header {
            background: linear-gradient(135deg, #BF0000 0%, #990000 100%);
            padding: 20px;
            border-radius: 10px;
            margin-bottom: 20px;
        }
        .main-header h1 {
            color: white !important;
            margin: 0;
        }
        .main-header p {
            color: #FFD4D4;
            margin: 5px 0 0 0;
        }
        .stats-card {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 10px;
            text-align: center;
        }
        .stats-value {
            font-size: 2em;
            font-weight: bold;
            color: #BF0000;
        }
        </style>
    """, unsafe_allow_html=True)

    # ãƒ˜ãƒƒãƒ€ãƒ¼
    st.markdown("""
        <div class="main-header">
            <h1>ğŸ›’ æ¥½å¤©å¸‚å ´ ã‚«ãƒ†ã‚´ãƒªæŠ½å‡ºãƒ„ãƒ¼ãƒ«</h1>
            <p>Rakuten Category Extractor</p>
        </div>
    """, unsafe_allow_html=True)

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆåˆæœŸåŒ–
    if "scraper" not in st.session_state:
        st.session_state.scraper = None
    if "is_running" not in st.session_state:
        st.session_state.is_running = False
    if "excel_data" not in st.session_state:
        st.session_state.excel_data = None
    if "log_messages" not in st.session_state:
        st.session_state.log_messages = []
    if "total_categories" not in st.session_state:
        st.session_state.total_categories = 0

    # å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
    with st.container():
        st.subheader("ğŸ“ è¨­å®š")

        col1, col2 = st.columns([3, 1])

        with col1:
            url = st.text_input(
                "ã‚«ãƒ†ã‚´ãƒªURL",
                value="https://www.rakuten.co.jp/category/101354/",
                help="ä¾‹: https://www.rakuten.co.jp/category/101354/ ï¼ˆDVDï¼‰"
            )

        with col2:
            depth = st.number_input(
                "å–å¾—éšå±¤æ•°",
                min_value=1,
                max_value=10,
                value=3,
                help="1ã€œ10éšå±¤ã¾ã§æŒ‡å®šå¯èƒ½"
            )

    # ãƒœã‚¿ãƒ³ã‚¨ãƒªã‚¢
    start_disabled = st.session_state.is_running
    start_clicked = st.button("ğŸš€ æŠ½å‡ºé–‹å§‹", disabled=start_disabled, type="primary")

    # æŠ½å‡ºå‡¦ç†
    if start_clicked and url:
        st.session_state.is_running = True
        st.session_state.excel_data = None
        st.session_state.log_messages = []
        st.session_state.total_categories = 0

        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼å®Ÿè¡Œ
        scraper = RakutenCategoryScraper()
        st.session_state.scraper = scraper

        progress_bar = st.progress(0)
        status_text = st.empty()
        log_container = st.empty()

        def update_progress(count, path):
            st.session_state.total_categories = count
            status_text.text(f"å–å¾—ä¸­: {count}ä»¶ | {' > '.join(path)}")

        try:
            with st.spinner("ã‚«ãƒ†ã‚´ãƒªã‚’å–å¾—ä¸­..."):
                categories = scraper.scrape(url, max_depth=depth, progress_callback=update_progress)

            st.session_state.log_messages = scraper.log_messages

            if categories:
                st.session_state.excel_data = scraper.export_to_excel()
                st.session_state.total_categories = len(categories)
                st.success(f"âœ… {len(categories)}ä»¶ã®ã‚«ãƒ†ã‚´ãƒªã‚’å–å¾—ã—ã¾ã—ãŸï¼")
            else:
                st.warning("ã‚«ãƒ†ã‚´ãƒªãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")

        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        finally:
            st.session_state.is_running = False
            st.rerun()

    elif start_clicked and not url:
        st.error("URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

    # çµ±è¨ˆæƒ…å ±
    st.divider()

    col1, col2 = st.columns(2)

    with col1:
        st.markdown(f"""
            <div class="stats-card">
                <p>å–å¾—ã‚«ãƒ†ã‚´ãƒªæ•°</p>
                <div class="stats-value">{st.session_state.total_categories}ä»¶</div>
            </div>
        """, unsafe_allow_html=True)

    with col2:
        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
        if st.session_state.excel_data:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            st.download_button(
                label="ğŸ“¥ Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=st.session_state.excel_data,
                file_name=f"rakuten_categories_{timestamp}.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                type="primary",
                use_container_width=True
            )

    # ãƒ­ã‚°è¡¨ç¤º
    if st.session_state.log_messages:
        with st.expander("ğŸ“‹ ãƒ­ã‚°", expanded=False):
            log_text = "\n".join(st.session_state.log_messages)
            st.code(log_text, language=None)


if __name__ == "__main__":
    main()
