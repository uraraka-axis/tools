#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Yahoo!ã‚·ãƒ§ãƒƒãƒ”ãƒ³ã‚° ã‚«ãƒ†ã‚´ãƒªæŠ½å‡ºãƒ„ãƒ¼ãƒ« - Streamlitç‰ˆ
Webãƒ–ãƒ©ã‚¦ã‚¶ã‹ã‚‰ä½¿ç”¨å¯èƒ½ãªãƒãƒ¼ã‚¸ãƒ§ãƒ³
"""

import streamlit as st
import requests
from bs4 import BeautifulSoup
from openpyxl import Workbook
from openpyxl.styles import Font, PatternFill, Alignment, Border, Side
from openpyxl.utils import get_column_letter
from typing import List, Dict
from dataclasses import dataclass, field
from datetime import datetime
from collections import defaultdict
import random
import time
import re
import json
import io


@dataclass
class Category:
    """ã‚«ãƒ†ã‚´ãƒªæƒ…å ±ã‚’ä¿æŒã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    name: str
    category_id: str
    url: str
    count: int
    level: int
    parent_path: List[str] = field(default_factory=list)


class YahooCategoryScraper:
    """Yahoo!ã‚·ãƒ§ãƒƒãƒ”ãƒ³ã‚°ã‚«ãƒ†ã‚´ãƒªã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ (requestsç‰ˆ)"""

    BASE_URL = "https://shopping.yahoo.co.jp"

    def __init__(self):
        self.session = requests.Session()
        # ã‚ˆã‚Šå®Œå…¨ãªãƒ–ãƒ©ã‚¦ã‚¶ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¨­å®šï¼ˆbotæ¤œå‡ºå›é¿ï¼‰
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Cache-Control': 'max-age=0',
            'Sec-Ch-Ua': '"Not A(Brand";v="99", "Google Chrome";v="121", "Chromium";v="121"',
            'Sec-Ch-Ua-Mobile': '?0',
            'Sec-Ch-Ua-Platform': '"Windows"',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Upgrade-Insecure-Requests': '1',
        })
        self.stop_flag = False
        self.categories: List[Category] = []
        self.root_category_name = ""
        self.root_category_id = ""
        self.total_requests = 0
        self.log_messages = []

        # å¾…æ©Ÿæ™‚é–“è¨­å®š
        self.min_delay = 1.5
        self.max_delay = 4.0

    def log(self, message: str):
        """ãƒ­ã‚°å‡ºåŠ›"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        log_message = f"[{timestamp}] {message}"
        self.log_messages.append(log_message)

    def stop(self):
        """å‡¦ç†ã‚’åœæ­¢"""
        self.stop_flag = True

    def random_delay(self):
        """ãƒ©ãƒ³ãƒ€ãƒ ãªå¾…æ©Ÿæ™‚é–“"""
        delay = random.uniform(self.min_delay, self.max_delay)
        if random.random() < 0.1:
            delay += random.uniform(1.0, 3.0)
        time.sleep(delay)

    def extract_category_id_from_url(self, url: str) -> str:
        """URLã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªIDãƒ‘ã‚¹ã‚’æŠ½å‡º"""
        match = re.search(r'/category/([\d/]+)/list', url)
        if match:
            return match.group(1).rstrip('/')
        return ""

    def get_last_category_id(self, category_path: str) -> str:
        """ã‚«ãƒ†ã‚´ãƒªãƒ‘ã‚¹ã‹ã‚‰æœ€å¾Œã®IDã‚’å–å¾—"""
        if '/' in category_path:
            return category_path.split('/')[-1]
        return category_path

    def fetch_page(self, url: str):
        """ãƒšãƒ¼ã‚¸ã‚’å–å¾—"""
        try:
            self.total_requests += 1

            if self.total_requests > 1:
                self.random_delay()

            response = self.session.get(url, timeout=30)
            response.raise_for_status()

            return BeautifulSoup(response.text, 'html.parser')

        except Exception as e:
            self.log(f"  âš ï¸ ãƒšãƒ¼ã‚¸å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def get_root_category_name(self, soup: BeautifulSoup) -> str:
        """ãƒ«ãƒ¼ãƒˆã‚«ãƒ†ã‚´ãƒªåã‚’å–å¾—"""
        h1 = soup.find('h1')
        if h1:
            name = h1.get_text(strip=True)
            name = re.sub(r'æ˜ åƒã‚½ãƒ•ãƒˆ$|ãŠã™ã™ã‚.*$', '', name).strip()
            return name
        return "ã‚«ãƒ†ã‚´ãƒª"

    def get_subcategories_from_page(self, soup: BeautifulSoup, current_category_id: str, is_root: bool = False) -> List[Dict]:
        """ãƒšãƒ¼ã‚¸ã‹ã‚‰ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªã‚’æŠ½å‡ºï¼ˆ__NEXT_DATA__ã®JSONã‹ã‚‰å–å¾—ï¼‰"""
        subcategories = []

        next_data_script = soup.find('script', id='__NEXT_DATA__')
        if not next_data_script:
            self.log("    [DEBUG] __NEXT_DATA__ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            # HTMLãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’è©¦ã™
            return self._extract_categories_from_html(soup, current_category_id)

        try:
            json_data = json.loads(next_data_script.string)

            # ãƒ‡ãƒãƒƒã‚°: JSONã®æ§‹é€ ã‚’ç¢ºèª
            props = json_data.get('props', {})
            page_props = props.get('pageProps', {})
            initial_state = page_props.get('initialState', {})

            self.log(f"    [DEBUG] props keys: {list(props.keys())[:5]}")
            self.log(f"    [DEBUG] pageProps keys: {list(page_props.keys())[:5]}")
            self.log(f"    [DEBUG] initialState keys: {list(initial_state.keys())[:5]}")

            categories_data = self._extract_categories_from_json(json_data)

            if not categories_data:
                self.log("    [DEBUG] JSONã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åˆ¥ã®ãƒ‘ã‚¹ã‚’è©¦ã™
                categories_data = self._extract_categories_fallback(json_data)
                if categories_data:
                    self.log(f"    [DEBUG] ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§ {len(categories_data)} ä»¶å–å¾—")

            # JSONã‹ã‚‰1ä»¶ä»¥ä¸‹ã®å ´åˆã¯HTMLãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’è©¦ã™
            if len(categories_data) <= 1:
                self.log("    [DEBUG] JSONçµæœãŒä¸ååˆ†ã€HTMLãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’è©¦è¡Œ")
                html_categories = self._extract_categories_from_html(soup, current_category_id)
                if len(html_categories) > len(categories_data):
                    self.log(f"    [DEBUG] HTMLã‹ã‚‰ {len(html_categories)} ä»¶å–å¾—")
                    return html_categories

            if not categories_data:
                return []

            self.log(f"    [DEBUG] JSONã‹ã‚‰ {len(categories_data)} ä»¶ã®ã‚«ãƒ†ã‚´ãƒªã‚’æ¤œå‡º")

            for cat_data in categories_data:
                name = cat_data.get('text', '')
                url = cat_data.get('url', '')
                count = cat_data.get('count', 0)

                if not name or not url:
                    continue

                category_path = self.extract_category_id_from_url(url)
                if not category_path:
                    continue

                last_id = self.get_last_category_id(category_path)

                if not url.startswith('http'):
                    url = self.BASE_URL + url

                url = re.sub(r'\?.*$', '', url)
                if not url.endswith('/list'):
                    url = url.rstrip('/') + '/list'

                subcategories.append({
                    'name': name,
                    'url': url,
                    'category_id': category_path,
                    'last_id': last_id,
                    'count': count
                })

        except json.JSONDecodeError as e:
            self.log(f"    [DEBUG] JSONè§£æã‚¨ãƒ©ãƒ¼: {e}")
            return []
        except Exception as e:
            self.log(f"    [DEBUG] ã‚«ãƒ†ã‚´ãƒªæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return []

        # é‡è¤‡é™¤å»
        seen = set()
        unique = []
        for cat in subcategories:
            if cat['last_id'] not in seen and cat['name']:
                seen.add(cat['last_id'])
                unique.append(cat)

        return unique

    def _extract_categories_from_json(self, json_data: dict) -> List[Dict]:
        """JSONãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªæƒ…å ±ã‚’æŠ½å‡º"""
        categories = []

        try:
            page_props = json_data.get('props', {}).get('pageProps', {})

            # ãƒ‘ã‚¹1: initialState.bff.advancedFilterçµŒç”±ï¼ˆå¾“æ¥æ–¹å¼ï¼‰
            initial_state = page_props.get('initialState', {})
            if initial_state:
                bff = initial_state.get('bff', {})
                advanced_filter = bff.get('advancedFilter', {})
                sections = advanced_filter.get('sections', {})
                category_section = sections.get('category', {})
                categories_data = category_section.get('categories', {})

                suggested = categories_data.get('suggestedCategories', [])
                if suggested:
                    categories.extend(suggested)

                toggle_items = categories_data.get('toggleAreaCategoryItems', [])
                if toggle_items:
                    categories.extend(toggle_items)

            # ãƒ‘ã‚¹2: ptahV2InitialDataçµŒç”±ï¼ˆæ–°æ–¹å¼ï¼‰
            if not categories:
                ptah_data = page_props.get('ptahV2InitialData', {})
                if ptah_data:
                    # ptahV2InitialDataãŒæ–‡å­—åˆ—ã®å ´åˆã¯JSONã¨ã—ã¦ãƒ‘ãƒ¼ã‚¹
                    if isinstance(ptah_data, str):
                        try:
                            ptah_data = json.loads(ptah_data)
                            self.log(f"    [DEBUG] ptahV2InitialData parsed from string")
                        except json.JSONDecodeError:
                            self.log(f"    [DEBUG] ptahV2InitialData is not valid JSON")
                            ptah_data = {}

                    if isinstance(ptah_data, dict):
                        self.log(f"    [DEBUG] ptahV2InitialData keys: {list(ptah_data.keys())[:10]}")
                        # ptahV2InitialDataå†…ã‚’æ¢ç´¢
                        categories = self._search_categories_in_ptah(ptah_data)

        except Exception as e:
            self.log(f"    [DEBUG] JSONæ§‹é€ è§£æã‚¨ãƒ©ãƒ¼: {e}")

        return categories

    def _search_categories_in_ptah(self, ptah_data: dict) -> List[Dict]:
        """ptahV2InitialDataå†…ã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªã‚’æ¢ç´¢"""
        categories = []

        def search(obj, depth=0):
            if depth > 15:
                return []
            found = []

            if isinstance(obj, dict):
                # advancedFilterã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ¢ã™
                if 'advancedFilter' in obj:
                    af = obj['advancedFilter']
                    if isinstance(af, dict) and 'sections' in af:
                        sections = af['sections']
                        if isinstance(sections, dict) and 'category' in sections:
                            cat_section = sections['category']
                            if isinstance(cat_section, dict) and 'categories' in cat_section:
                                cat_data = cat_section['categories']
                                if isinstance(cat_data, dict):
                                    suggested = cat_data.get('suggestedCategories', [])
                                    toggle = cat_data.get('toggleAreaCategoryItems', [])
                                    found.extend(suggested)
                                    found.extend(toggle)
                                    if found:
                                        return found

                # suggestedCategoriesã‚’ç›´æ¥æ¢ã™
                if 'suggestedCategories' in obj:
                    items = obj['suggestedCategories']
                    if isinstance(items, list) and items:
                        if isinstance(items[0], dict) and 'text' in items[0]:
                            found.extend(items)
                            toggle = obj.get('toggleAreaCategoryItems', [])
                            found.extend(toggle)
                            return found

                # å†å¸°æ¢ç´¢
                for v in obj.values():
                    result = search(v, depth + 1)
                    if result:
                        return result

            elif isinstance(obj, list):
                for item in obj:
                    result = search(item, depth + 1)
                    if result:
                        return result

            return found

        try:
            categories = search(ptah_data)
        except Exception as e:
            self.log(f"    [DEBUG] ptahæ¢ç´¢ã‚¨ãƒ©ãƒ¼: {e}")

        return categories

    def _extract_categories_fallback(self, json_data: dict) -> List[Dict]:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åˆ¥ã®ãƒ‘ã‚¹ã§ã‚«ãƒ†ã‚´ãƒªã‚’æ¢ã™ï¼ˆsuggestedCategories + toggleAreaCategoryItemsï¼‰"""
        categories = []

        def find_categories(obj, depth=0):
            """å†å¸°çš„ã«ã‚«ãƒ†ã‚´ãƒªé…åˆ—ã‚’æ¢ã™ï¼ˆä¸¡æ–¹ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™ï¼‰"""
            if depth > 10:
                return None
            if isinstance(obj, dict):
                # suggestedCategories ã‚’æ¢ã™ï¼ˆtoggleAreaCategoryItemsã‚‚ä¸€ç·’ã«å–å¾—ï¼‰
                if 'suggestedCategories' in obj:
                    result = []
                    suggested = obj['suggestedCategories']
                    if isinstance(suggested, list):
                        result.extend(suggested)
                    # toggleAreaCategoryItems ã‚‚å–å¾—ï¼ˆã€Œã‚‚ã£ã¨è¦‹ã‚‹ã€ã®ã‚«ãƒ†ã‚´ãƒªï¼‰
                    toggle_items = obj.get('toggleAreaCategoryItems', [])
                    if isinstance(toggle_items, list):
                        result.extend(toggle_items)
                    if result:
                        return result
                # categories ã‚­ãƒ¼ã‚’æ¢ã™
                if 'categories' in obj and isinstance(obj['categories'], dict):
                    cat_data = obj['categories']
                    result = []
                    suggested = cat_data.get('suggestedCategories', [])
                    if isinstance(suggested, list):
                        result.extend(suggested)
                    toggle_items = cat_data.get('toggleAreaCategoryItems', [])
                    if isinstance(toggle_items, list):
                        result.extend(toggle_items)
                    if result:
                        return result
                # categories ãŒé…åˆ—ã®å ´åˆ
                if 'categories' in obj and isinstance(obj['categories'], list):
                    items = obj['categories']
                    if items and isinstance(items[0], dict) and 'text' in items[0]:
                        return items
                # å†å¸°çš„ã«æ¢ç´¢
                for v in obj.values():
                    result = find_categories(v, depth + 1)
                    if result:
                        return result
            elif isinstance(obj, list) and obj:
                for item in obj:
                    result = find_categories(item, depth + 1)
                    if result:
                        return result
            return None

        try:
            found = find_categories(json_data)
            if found:
                categories = found
                self.log(f"    [DEBUG] ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§ suggestedCategories + toggleAreaCategoryItems ã‚’å–å¾—")
        except Exception as e:
            self.log(f"    [DEBUG] ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¢ç´¢ã‚¨ãƒ©ãƒ¼: {e}")

        return categories

    def _extract_categories_from_html(self, soup: BeautifulSoup, current_category_id: str) -> List[Dict]:
        """HTMLã‹ã‚‰ç›´æ¥ã‚«ãƒ†ã‚´ãƒªãƒªãƒ³ã‚¯ã‚’æŠ½å‡ºï¼ˆJSONãŒä¸ååˆ†ãªå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
        subcategories = []
        seen_ids = set()

        # ç¾åœ¨ã®ã‚«ãƒ†ã‚´ãƒªIDã®æœ€å¾Œã®éƒ¨åˆ†ã‚’å–å¾—ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç”¨ï¼‰
        current_last_id = self.get_last_category_id(current_category_id) if current_category_id else ""

        # ç¾åœ¨ã®ã‚«ãƒ†ã‚´ãƒªãƒ‘ã‚¹ã®éšå±¤æ•°ï¼ˆè¦ªã‚«ãƒ†ã‚´ãƒªé™¤å¤–ç”¨ï¼‰
        current_depth = len(current_category_id.split('/')) if current_category_id else 0

        # ã‚«ãƒ†ã‚´ãƒªãƒªãƒ³ã‚¯ã®ãƒ‘ã‚¿ãƒ¼ãƒ³: /category/æ•°å­—/list ã¾ãŸã¯ /category/æ•°å­—/æ•°å­—/list ãªã©
        category_pattern = re.compile(r'/category/([\d/]+)/list')

        # ã™ã¹ã¦ã®ãƒªãƒ³ã‚¯ã‚’æ¢ç´¢
        for link in soup.find_all('a', href=True):
            href = link.get('href', '')
            match = category_pattern.search(href)
            if not match:
                continue

            category_path = match.group(1).rstrip('/')
            last_id = self.get_last_category_id(category_path)

            # ã‚«ãƒ†ã‚´ãƒªãƒ‘ã‚¹ã®éšå±¤æ•°
            link_depth = len(category_path.split('/'))

            # è¦ªã‚«ãƒ†ã‚´ãƒªï¼ˆéšå±¤ãŒæµ…ã„ï¼‰ã¯é™¤å¤–
            if link_depth <= current_depth:
                continue

            # è‡ªåˆ†è‡ªèº«ã¯é™¤å¤–
            if last_id == current_last_id:
                continue

            # é‡è¤‡é™¤å»
            if last_id in seen_ids:
                continue
            seen_ids.add(last_id)

            # ãƒªãƒ³ã‚¯ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
            name = link.get_text(strip=True)
            if not name:
                continue

            # æ•°å­—ã ã‘ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆä»¶æ•°è¡¨ç¤ºãªã©ï¼‰
            if name.isdigit() or re.match(r'^[\d,]+ä»¶?$', name):
                continue

            # ã€Œã‚‚ã£ã¨è¦‹ã‚‹ã€ã€Œã™ã¹ã¦è¦‹ã‚‹ã€ãªã©ã¯ã‚¹ã‚­ãƒƒãƒ—
            if name in ['ã‚‚ã£ã¨è¦‹ã‚‹', 'ã™ã¹ã¦è¦‹ã‚‹', 'è©³ç´°ã‚’è¦‹ã‚‹', 'é–‰ã˜ã‚‹']:
                continue

            url = href
            if not url.startswith('http'):
                url = self.BASE_URL + url

            url = re.sub(r'\?.*$', '', url)
            if not url.endswith('/list'):
                url = url.rstrip('/') + '/list'

            # ä»¶æ•°ã‚’å–å¾—ï¼ˆãƒªãƒ³ã‚¯ã®è¿‘ãã«ã‚ã‚‹æ•°å­—ï¼‰
            count = 0
            try:
                count_text = link.find_next(string=re.compile(r'[\d,]+ä»¶?'))
                if count_text:
                    count_match = re.search(r'([\d,]+)', str(count_text))
                    if count_match:
                        count_str = count_match.group(1).replace(',', '')
                        if count_str:
                            count = int(count_str)
            except (ValueError, AttributeError):
                count = 0

            subcategories.append({
                'name': name,
                'url': url,
                'category_id': category_path,
                'last_id': last_id,
                'count': count
            })

        self.log(f"    [DEBUG] HTMLã‹ã‚‰ {len(subcategories)} ä»¶ã®ã‚«ãƒ†ã‚´ãƒªãƒªãƒ³ã‚¯ã‚’æ¤œå‡º")
        return subcategories

    def scrape_categories_recursive(
        self,
        url: str,
        level: int = 0,
        parent_path: List[str] = None,
        max_depth: int = 5,
        progress_callback=None
    ):
        """ã‚«ãƒ†ã‚´ãƒªã‚’å†å¸°çš„ã«å–å¾—"""
        if parent_path is None:
            parent_path = []

        if level > max_depth or self.stop_flag:
            return

        indent = "  " * level
        self.log(f"{indent}ğŸ“‚ å–å¾—ä¸­: {url}")

        soup = self.fetch_page(url)
        if not soup:
            return

        current_id = self.extract_category_id_from_url(url)

        is_root = (level == 0)
        if is_root:
            self.root_category_name = self.get_root_category_name(soup)
            self.root_category_id = current_id
            self.log(f"ğŸ“Œ ãƒ«ãƒ¼ãƒˆã‚«ãƒ†ã‚´ãƒª: {self.root_category_name} (ID: {current_id})")

        subcategories = self.get_subcategories_from_page(soup, current_id, is_root=is_root)

        self.log(f"{indent}  â†’ {len(subcategories)}ä»¶ã®ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªã‚’ç™ºè¦‹")

        for subcat in subcategories:
            if self.stop_flag:
                break

            cat = Category(
                name=subcat['name'],
                category_id=subcat['category_id'],
                url=subcat['url'],
                count=subcat['count'],
                level=level + 1,
                parent_path=parent_path.copy()
            )
            self.categories.append(cat)

            self.log(f"{indent}  âœ“ {subcat['name']} ({subcat['count']:,}ä»¶) [ID: {subcat['last_id']}]")

            # é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if progress_callback:
                progress_callback(len(self.categories), parent_path + [subcat['name']])

            # å†å¸°
            if level + 1 < max_depth:
                new_parent_path = parent_path + [subcat['name']]
                self.scrape_categories_recursive(
                    subcat['url'],
                    level + 1,
                    new_parent_path,
                    max_depth,
                    progress_callback
                )

    def scrape(self, start_url: str, max_depth: int = 5, progress_callback=None) -> List[Category]:
        """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹"""
        self.stop_flag = False
        self.categories = []
        self.total_requests = 0
        self.log_messages = []

        self.log("=" * 50)
        self.log("ğŸ›’ Yahoo!ã‚·ãƒ§ãƒƒãƒ”ãƒ³ã‚° ã‚«ãƒ†ã‚´ãƒªæŠ½å‡ºé–‹å§‹")
        self.log("=" * 50)
        self.log(f"ğŸ”— URL: {start_url}")
        self.log(f"ğŸ“Š æœ€å¤§å–å¾—éšå±¤: {max_depth}")
        self.log("")

        self.scrape_categories_recursive(start_url, max_depth=max_depth, progress_callback=progress_callback)

        if not self.stop_flag:
            self.log("")
            self.log(f"âœ… åˆè¨ˆ {len(self.categories)} ã‚«ãƒ†ã‚´ãƒªã‚’å–å¾—ã—ã¾ã—ãŸ")
            self.log(f"ğŸ“¡ ç·ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°: {self.total_requests}")

        return self.categories

    def export_to_excel(self) -> bytes:
        """Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚¤ãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ ã§å‡ºåŠ›"""
        if not self.categories:
            return None

        wb = Workbook()
        ws = wb.active
        ws.title = "ã‚¸ãƒ£ãƒ³ãƒ«ä¸€è¦§"

        base_font = Font(name="Meiryo UI", size=10)
        header_font = Font(name="Meiryo UI", bold=True, color="FFFFFF", size=11)
        header_fill = PatternFill("solid", fgColor="ff0033")
        header_alignment = Alignment(horizontal="center", vertical="center", wrap_text=True)
        title_font = Font(name="Meiryo UI", bold=True, size=14, color="ff0033")

        thin_border = Border(
            left=Side(style='thin', color='595959'),
            right=Side(style='thin', color='595959'),
            top=Side(style='thin', color='595959'),
            bottom=Side(style='thin', color='595959')
        )

        max_level = max((cat.level for cat in self.categories), default=1)

        level_counts = defaultdict(int)
        for cat in self.categories:
            level_counts[cat.level] += 1

        summary_level_col = 2 + max_level + 4
        summary_count_col = 2 + max_level + 5

        title_col_end = get_column_letter(summary_count_col)
        ws.merge_cells(f'B1:{title_col_end}1')
        ws['B1'] = f"ã€Yahoo!ã‚·ãƒ§ãƒƒãƒ”ãƒ³ã‚°ã€‘{self.root_category_name}ã®ã‚¸ãƒ£ãƒ³ãƒ«ä¸€è¦§"
        ws['B1'].font = title_font
        ws['B1'].alignment = Alignment(horizontal="center", vertical="center")
        ws.row_dimensions[1].height = 28
        ws.row_dimensions[2].height = 8

        headers = ["#", "ã‚¸ãƒ£ãƒ³ãƒ«1"]
        for i in range(max_level):
            headers.append(f"ã‚¸ãƒ£ãƒ³ãƒ«{i + 2}")
        headers.append("ã‚«ãƒ†ã‚´ãƒªID")
        headers.append("ãƒšãƒ¼ã‚¸URL")

        for col, header in enumerate(headers, 1):
            cell = ws.cell(row=3, column=col, value=header)
            cell.font = header_font
            cell.fill = header_fill
            cell.alignment = header_alignment
            cell.border = thin_border

        ws.row_dimensions[3].height = 24

        summary_level_header = ws.cell(row=3, column=summary_level_col, value="ãƒ¬ãƒ™ãƒ«")
        summary_level_header.font = header_font
        summary_level_header.fill = header_fill
        summary_level_header.alignment = header_alignment
        summary_level_header.border = thin_border

        summary_count_header = ws.cell(row=3, column=summary_count_col, value="ã‚«ãƒ†ã‚´ãƒªæ•°")
        summary_count_header.font = header_font
        summary_count_header.fill = header_fill
        summary_count_header.alignment = header_alignment
        summary_count_header.border = thin_border

        prev_values = [""] * (max_level + 1)

        for idx, cat in enumerate(self.categories, 1):
            row = idx + 3

            current_values = [self.root_category_name] + [""] * max_level

            for i, parent_name in enumerate(cat.parent_path):
                if i < max_level:
                    current_values[i + 1] = parent_name

            if cat.level <= max_level:
                current_values[cat.level] = cat.name

            cell = ws.cell(row=row, column=1, value=idx)
            cell.border = thin_border
            cell.font = base_font

            for col, value in enumerate(current_values, 2):
                cell = ws.cell(row=row, column=col)

                show_value = value
                if idx > 1 and col - 2 < len(prev_values):
                    if value == prev_values[col - 2]:
                        has_change = any(
                            current_values[j] != prev_values[j]
                            for j in range(col - 1, len(current_values))
                            if j < len(prev_values)
                        )
                        if not has_change:
                            show_value = ""

                cell.value = show_value
                cell.border = thin_border
                cell.font = base_font

            id_col = 2 + max_level + 1
            last_id = self.get_last_category_id(cat.category_id)
            id_cell = ws.cell(row=row, column=id_col, value=last_id)
            id_cell.border = thin_border
            id_cell.font = base_font

            url_col = 2 + max_level + 2
            url_cell = ws.cell(row=row, column=url_col, value=cat.url)
            url_cell.hyperlink = cat.url
            url_cell.style = "Hyperlink"
            url_cell.border = thin_border
            url_cell.font = Font(name="Meiryo UI", size=10, color="0563C1", underline="single")

            prev_values = current_values.copy()

        summary_row = 4

        level_cell = ws.cell(row=summary_row, column=summary_level_col, value="ã‚¸ãƒ£ãƒ³ãƒ«1")
        level_cell.border = thin_border
        level_cell.font = base_font

        count_cell = ws.cell(row=summary_row, column=summary_count_col, value=1)
        count_cell.border = thin_border
        count_cell.font = base_font
        count_cell.alignment = Alignment(horizontal="right")

        summary_row += 1

        for level in sorted(level_counts.keys()):
            level_cell = ws.cell(row=summary_row, column=summary_level_col, value=f"ã‚¸ãƒ£ãƒ³ãƒ«{level + 1}")
            level_cell.border = thin_border
            level_cell.font = base_font

            count_cell = ws.cell(row=summary_row, column=summary_count_col, value=level_counts[level])
            count_cell.border = thin_border
            count_cell.font = base_font
            count_cell.alignment = Alignment(horizontal="right")

            summary_row += 1

        total_cell = ws.cell(row=summary_row, column=summary_level_col, value="åˆè¨ˆ")
        total_cell.border = thin_border
        total_cell.font = Font(name="Meiryo UI", size=10, bold=True)

        total_count_cell = ws.cell(row=summary_row, column=summary_count_col, value=len(self.categories) + 1)
        total_count_cell.border = thin_border
        total_count_cell.font = Font(name="Meiryo UI", size=10, bold=True)
        total_count_cell.alignment = Alignment(horizontal="right")

        ws.column_dimensions['A'].width = 6
        ws.column_dimensions['B'].width = 18
        for i in range(max_level):
            col_letter = get_column_letter(3 + i)
            ws.column_dimensions[col_letter].width = 22
        ws.column_dimensions[get_column_letter(3 + max_level)].width = 18
        ws.column_dimensions[get_column_letter(4 + max_level)].width = 50
        ws.column_dimensions[get_column_letter(5 + max_level)].width = 3
        ws.column_dimensions[get_column_letter(summary_level_col)].width = 12
        ws.column_dimensions[get_column_letter(summary_count_col)].width = 12

        ws.freeze_panes = 'A4'

        # ãƒã‚¤ãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ ã¨ã—ã¦å‡ºåŠ›
        output = io.BytesIO()
        wb.save(output)
        output.seek(0)
        return output.getvalue()


def check_password():
    """ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰èªè¨¼"""

    # secretsã«ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
    if "password" not in st.secrets:
        # ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰æœªè¨­å®šã®å ´åˆã¯ãã®ã¾ã¾ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½
        return True

    correct_password = st.secrets["password"]

    if "authenticated" not in st.session_state:
        st.session_state.authenticated = False

    if st.session_state.authenticated:
        return True

    st.markdown("""
        <style>
        .login-container {
            max-width: 400px;
            margin: 100px auto;
            padding: 40px;
            background: white;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        </style>
    """, unsafe_allow_html=True)

    st.title("ğŸ” ãƒ­ã‚°ã‚¤ãƒ³")
    st.write("ã“ã®ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒå¿…è¦ã§ã™ã€‚")

    password = st.text_input("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰", type="password", key="password_input")

    if st.button("ãƒ­ã‚°ã‚¤ãƒ³", type="primary"):
        if password == correct_password:
            st.session_state.authenticated = True
            st.rerun()
        else:
            st.error("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒé•ã„ã¾ã™")

    return False


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""

    st.set_page_config(
        page_title="Yahoo!ã‚·ãƒ§ãƒƒãƒ”ãƒ³ã‚° ã‚«ãƒ†ã‚´ãƒªæŠ½å‡ºãƒ„ãƒ¼ãƒ«",
        page_icon="ğŸ›’",
        layout="wide"
    )

    # ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰èªè¨¼
    if not check_password():
        return

    # ã‚«ã‚¹ã‚¿ãƒ CSS
    st.markdown("""
        <style>
        .main-header {
            background: linear-gradient(135deg, #ff0033 0%, #cc0029 100%);
            padding: 20px;
            border-radius: 10px;
            margin-bottom: 20px;
        }
        .main-header h1 {
            color: white !important;
            margin: 0;
        }
        .main-header p {
            color: #FFE0B2;
            margin: 5px 0 0 0;
        }
        .stats-card {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 10px;
            text-align: center;
        }
        .stats-value {
            font-size: 2em;
            font-weight: bold;
            color: #ff0033;
        }
        </style>
    """, unsafe_allow_html=True)

    # ãƒ˜ãƒƒãƒ€ãƒ¼
    st.markdown("""
        <div class="main-header">
            <h1>ğŸ›’ Yahoo!ã‚·ãƒ§ãƒƒãƒ”ãƒ³ã‚° ã‚«ãƒ†ã‚´ãƒªæŠ½å‡ºãƒ„ãƒ¼ãƒ«</h1>
            <p>Yahoo! Shopping Category Extractor</p>
        </div>
    """, unsafe_allow_html=True)

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆåˆæœŸåŒ–
    if "scraper" not in st.session_state:
        st.session_state.scraper = None
    if "is_running" not in st.session_state:
        st.session_state.is_running = False
    if "excel_data" not in st.session_state:
        st.session_state.excel_data = None
    if "log_messages" not in st.session_state:
        st.session_state.log_messages = []
    if "total_categories" not in st.session_state:
        st.session_state.total_categories = 0

    # å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
    with st.container():
        st.subheader("ğŸ“ è¨­å®š")

        col1, col2 = st.columns([3, 1])

        with col1:
            url = st.text_input(
                "ã‚«ãƒ†ã‚´ãƒªURL",
                value="https://shopping.yahoo.co.jp/category/2517/list",
                help="ä¾‹: https://shopping.yahoo.co.jp/category/2517/list ï¼ˆDVDã€æ˜ åƒã‚½ãƒ•ãƒˆï¼‰"
            )

        with col2:
            depth = st.number_input(
                "å–å¾—éšå±¤æ•°",
                min_value=1,
                max_value=10,
                value=3,
                help="1ã€œ10éšå±¤ã¾ã§æŒ‡å®šå¯èƒ½"
            )

    # ãƒœã‚¿ãƒ³ã‚¨ãƒªã‚¢
    start_disabled = st.session_state.is_running
    start_clicked = st.button("ğŸš€ æŠ½å‡ºé–‹å§‹", disabled=start_disabled, type="primary")

    # æŠ½å‡ºå‡¦ç†
    if start_clicked and url:
        st.session_state.is_running = True
        st.session_state.excel_data = None
        st.session_state.log_messages = []
        st.session_state.total_categories = 0

        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼å®Ÿè¡Œ
        scraper = YahooCategoryScraper()
        st.session_state.scraper = scraper

        progress_bar = st.progress(0)
        status_text = st.empty()
        log_container = st.empty()

        def update_progress(count, path):
            st.session_state.total_categories = count
            status_text.text(f"å–å¾—ä¸­: {count}ä»¶ | {' > '.join(path)}")

        try:
            with st.spinner("ã‚«ãƒ†ã‚´ãƒªã‚’å–å¾—ä¸­..."):
                categories = scraper.scrape(url, max_depth=depth, progress_callback=update_progress)

            st.session_state.log_messages = scraper.log_messages

            if categories:
                st.session_state.excel_data = scraper.export_to_excel()
                st.session_state.total_categories = len(categories)
                st.success(f"âœ… {len(categories)}ä»¶ã®ã‚«ãƒ†ã‚´ãƒªã‚’å–å¾—ã—ã¾ã—ãŸï¼")
            else:
                st.warning("ã‚«ãƒ†ã‚´ãƒªãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")

        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        finally:
            st.session_state.is_running = False
            st.rerun()

    elif start_clicked and not url:
        st.error("URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

    # çµ±è¨ˆæƒ…å ±
    st.divider()

    col1, col2 = st.columns(2)

    with col1:
        st.markdown(f"""
            <div class="stats-card">
                <p>å–å¾—ã‚«ãƒ†ã‚´ãƒªæ•°</p>
                <div class="stats-value">{st.session_state.total_categories}ä»¶</div>
            </div>
        """, unsafe_allow_html=True)

    with col2:
        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
        if st.session_state.excel_data:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            st.download_button(
                label="ğŸ“¥ Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=st.session_state.excel_data,
                file_name=f"yahoo_categories_{timestamp}.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                type="primary",
                use_container_width=True
            )

    # ãƒ­ã‚°è¡¨ç¤º
    if st.session_state.log_messages:
        with st.expander("ğŸ“‹ ãƒ­ã‚°", expanded=False):
            log_text = "\n".join(st.session_state.log_messages)
            st.code(log_text, language=None)


if __name__ == "__main__":
    main()
