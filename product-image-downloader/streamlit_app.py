#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import streamlit as st
import requests
from bs4 import BeautifulSoup
from openpyxl import load_workbook
import time
from datetime import datetime
import random
import re
import os
import shutil
import tempfile
import zipfile
from pathlib import Path
from io import BytesIO

# ===== ãƒšãƒ¼ã‚¸è¨­å®š =====
st.set_page_config(page_title="å•†å“ç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ€ãƒ¼", page_icon="ğŸ“¦", layout="wide")


# ===== ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰èªè¨¼ =====
def check_password():
    if "authenticated" not in st.session_state:
        st.session_state.authenticated = False
    if st.session_state.authenticated:
        return True
    password_input = st.text_input("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", type="password")
    if password_input:
        if password_input == st.secrets.get("password", ""):
            st.session_state.authenticated = True
            st.rerun()
        else:
            st.error("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“")
    return False


if not check_password():
    st.stop()


# ===== å®šæ•° =====
SURUGAYA_SEARCH_URL = "https://www.suruga-ya.jp/kaitori/search_buy"
BOOKOFF_BASE_URL = "https://shopping.bookoff.co.jp/search/keyword/"
NO_IMAGE_PATTERNS = ['item_ll.gif', 'no_image', 'noimage', 'no-image', 'now_printing']
MIN_FILE_SIZE = 2 * 1024  # 2KB
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
                  '(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
}


# ===== Selenium ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— =====
def setup_driver():
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.chrome.service import Service

    options = Options()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--disable-gpu')
    options.add_argument('--log-level=3')
    options.add_argument('--disable-blink-features=AutomationControlled')

    # Streamlit Cloud (Debian) ã®ã‚·ã‚¹ãƒ†ãƒ  Chromium ã‚’æ¤œç´¢
    for chrome_path in ['/usr/bin/chromium-browser', '/usr/bin/chromium', '/usr/bin/google-chrome']:
        if os.path.exists(chrome_path):
            options.binary_location = chrome_path
            break

    driver = None
    for driver_path in ['/usr/bin/chromedriver', '/usr/lib/chromium-browser/chromedriver',
                        '/usr/lib/chromium/chromedriver']:
        if os.path.exists(driver_path):
            service = Service(driver_path)
            driver = webdriver.Chrome(service=service, options=options)
            break

    if driver is None:
        # ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒ: webdriver-manager ã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        from webdriver_manager.chrome import ChromeDriverManager
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=options)

    # é§¿æ²³å±‹ã‚»ãƒ¼ãƒ•ã‚µãƒ¼ãƒè¨­å®š
    driver.get("https://www.suruga-ya.jp/")
    driver.add_cookie({'name': 'safe_search_option', 'value': '3', 'domain': '.suruga-ya.jp'})
    return driver


# ===== ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° =====
def is_no_image(url: str) -> bool:
    url_lower = url.lower()
    return any(p in url_lower for p in NO_IMAGE_PATTERNS)


def sanitize(name) -> str | None:
    if not name or str(name).strip() == "":
        return None
    return re.sub(r'[<>:"/\\|?*]', '_', str(name)).strip('. ')


def build_genre_path(h_dir: Path, genres: list) -> Path:
    valid_genres = [g for g in genres if g is not None]
    if not valid_genres:
        return h_dir / "æœªåˆ†é¡"
    target_path = h_dir
    for genre in valid_genres:
        target_path = target_path / genre
    return target_path


# ===== ç”»åƒå–å¾—é–¢æ•° =====
def get_amazon_images(driver, asin: str, main_only: bool) -> list:
    url = f"https://www.amazon.co.jp/dp/{asin}"
    try:
        driver.get(url)
        time.sleep(random.uniform(2, 3))
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        images = []

        main_img = soup.find('img', {'id': 'landingImage'})
        if main_img:
            src = main_img.get('data-old-hires') or main_img.get('src')
            if src:
                src = re.sub(r'_AC_[A-Z]{2}\d+_', '_AC_SL1500_', src)
                images.append(src)

        if not main_only:
            alt_div = soup.find('div', {'id': 'altImages'})
            if alt_div:
                for thumb in alt_div.find_all('img'):
                    t_src = thumb.get('src')
                    if t_src and 'video' not in t_src.lower():
                        h_res = re.sub(r'_AC_[A-Z]{2}\d+,?\d*_', '_AC_SL1500_', t_src)
                        h_res = re.sub(r'\._[A-Z]{2}\d+,?\d*_\.', '._SL1500_.', h_res)
                        if h_res not in images and not is_no_image(h_res):
                            images.append(h_res)
        return images
    except Exception:
        return []


def get_surugaya_images(driver, jan: str) -> list:
    url = f"{SURUGAYA_SEARCH_URL}?search_word={jan}&key_flag=1"
    try:
        driver.get(url)
        time.sleep(2)
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        title_a = soup.select_one('div.title a')
        if not title_a:
            return []

        detail_url = title_a['href']
        if detail_url.startswith('/'):
            detail_url = "https://www.suruga-ya.jp" + detail_url

        driver.get(detail_url)
        time.sleep(2)
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        img_up = soup.find('div', {'id': 'imgUp'})
        if img_up and img_up.find('a'):
            img_url = img_up.find('a')['href']
            if img_url.startswith('/'):
                img_url = "https://www.suruga-ya.jp" + img_url
            return [img_url]
    except Exception:
        pass
    return []


def get_bookoff_images(driver, jan: str) -> list:
    url = f"{BOOKOFF_BASE_URL}{jan}"
    try:
        driver.get(url)
        time.sleep(2)
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        img_tag = soup.select_one('img.js-gridImg, .productItem__image img')
        if img_tag and img_tag.get('src'):
            img_url = img_tag['src'].replace('/SS/', '/LL/').replace('SS.jpg', 'LL.jpg')
            return [img_url]
    except Exception:
        pass
    return []


# ===== ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼†ãƒ•ã‚£ãƒ«ã‚¿ =====
def download_and_filter_images(session, images, base_fname, h_dir, f_dir, genres, main_only):
    valid_images = []
    for idx, url in enumerate(images):
        if idx > 0 and main_only:
            break
        try:
            resp = session.get(url, timeout=30, headers=HEADERS)
            if resp.status_code == 200:
                content = resp.content
                if len(content) > MIN_FILE_SIZE:
                    valid_images.append(content)
        except Exception:
            pass

    if not valid_images:
        return 0

    target_path = build_genre_path(h_dir, genres)
    target_path.mkdir(parents=True, exist_ok=True)

    saved_count = 0
    for idx, content in enumerate(valid_images):
        suffix = "" if idx == 0 else f"_{idx}"
        fname = f"{base_fname}{suffix}.jpg"
        save_h = target_path / fname
        save_f = f_dir / fname
        try:
            with open(save_h, 'wb') as f:
                f.write(content)
            shutil.copy2(save_h, save_f)
            saved_count += 1
        except Exception:
            pass
    return saved_count


# ===== ZIP ä½œæˆ =====
def create_zip(base_dir: Path) -> BytesIO:
    zip_buffer = BytesIO()
    with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zf:
        for root, _dirs, files in os.walk(base_dir):
            for file in files:
                file_path = Path(root) / file
                arcname = file_path.relative_to(base_dir)
                zf.write(file_path, arcname)
    zip_buffer.seek(0)
    return zip_buffer


# ===== ãƒ¡ã‚¤ãƒ³å‡¦ç† =====
def process(uploaded_file, main_only):
    session = requests.Session()

    with tempfile.TemporaryDirectory() as tmp_dir:
        tmp_dir = Path(tmp_dir)
        excel_path = tmp_dir / uploaded_file.name
        with open(excel_path, 'wb') as f:
            f.write(uploaded_file.getvalue())

        wb = load_workbook(excel_path)
        ws = wb.active

        h_dir = tmp_dir / "1_éšå±¤ç®¡ç†"
        f_dir = tmp_dir / "2_ä¸€æ‹¬ç®¡ç†"
        h_dir.mkdir(exist_ok=True)
        f_dir.mkdir(exist_ok=True)

        rows = [r for r in range(2, ws.max_row + 1) if ws[f'C{r}'].value or ws[f'D{r}'].value]
        total = len(rows)

        if total == 0:
            st.warning("å‡¦ç†å¯¾è±¡ã®è¡ŒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆCåˆ—ã¾ãŸã¯Dåˆ—ã«ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™ï¼‰")
            return None

        # ãƒ–ãƒ©ã‚¦ã‚¶èµ·å‹•
        with st.status("å‡¦ç†ä¸­...", expanded=True) as status:
            status.update(label="ãƒ–ãƒ©ã‚¦ã‚¶ã‚’èµ·å‹•ä¸­...")
            try:
                driver = setup_driver()
            except Exception as e:
                st.error(f"ãƒ–ãƒ©ã‚¦ã‚¶èµ·å‹•ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                return None

            progress_bar = st.progress(0)
            status_text = st.empty()
            log_area = st.empty()

            stats = {'total': total, 'success': 0, 'not_found': 0}
            logs = []

            try:
                for i, r in enumerate(rows, 1):
                    progress_bar.progress(i / total)
                    status.update(label=f"å‡¦ç†ä¸­: {i} / {total} ({i / total * 100:.1f}%)")
                    status_text.text(f"å‡¦ç†ä¸­: {i} / {total}")

                    seq = str(ws[f'B{r}'].value or "0").zfill(6)
                    jan = str(ws[f'C{r}'].value or "").strip()
                    asin = str(ws[f'D{r}'].value or "").strip()
                    genres = [sanitize(ws[f'{col}{r}'].value) for col in ['F', 'G', 'H', 'I']]
                    shelf = sanitize(ws[f'K{r}'].value) or "00"
                    base_code = sanitize(ws[f'M{r}'].value) or "XX"
                    base_fname = f"{shelf}-{base_code}-{seq}"

                    product_name = str(ws[f'E{r}'].value or "").strip()
                    if len(product_name) > 30:
                        product_name = product_name[:30] + "..."

                    images = []
                    source_site = ""

                    # Amazon â†’ é§¿æ²³å±‹ â†’ ãƒ–ãƒƒã‚¯ã‚ªãƒ•
                    if asin and asin not in ["-", ""]:
                        images = get_amazon_images(driver, asin, main_only)
                        if images:
                            source_site = "Amazon"

                    if not images and jan:
                        images = get_surugaya_images(driver, jan)
                        if images:
                            source_site = "é§¿æ²³å±‹"

                    if not images and jan:
                        images = get_bookoff_images(driver, jan)
                        if images:
                            source_site = "ãƒ–ãƒƒã‚¯ã‚ªãƒ•"

                    timestamp = datetime.now().strftime("%H:%M:%S")

                    if images:
                        downloaded_count = download_and_filter_images(
                            session, images, base_fname, h_dir, f_dir, genres, main_only
                        )
                        if downloaded_count > 0:
                            ws[f'J{r}'].value = downloaded_count
                            stats['success'] += 1
                            log_msg = f"[{timestamp}] [{i}/{total}] âœ… {source_site} / {product_name} / ç”»åƒ{downloaded_count}æš"
                        else:
                            stats['not_found'] += 1
                            log_msg = f"[{timestamp}] [{i}/{total}] âš ï¸ {source_site} / {product_name} / æœ‰åŠ¹ãªç”»åƒãªã—"
                    else:
                        stats['not_found'] += 1
                        log_msg = f"[{timestamp}] [{i}/{total}] âŒ å–å¾—å¤±æ•— / {product_name}"

                    logs.append(log_msg)
                    log_area.code("\n".join(logs[-50:]))  # ç›´è¿‘50ä»¶è¡¨ç¤º

                    time.sleep(random.uniform(0.5, 1.0))

            finally:
                driver.quit()

            status.update(label="âœ… å‡¦ç†å®Œäº†", state="complete", expanded=False)

        # Excel ä¿å­˜
        wb.save(excel_path)

        # ZIP ä½œæˆï¼ˆãƒ¡ãƒ¢ãƒªä¸Šã«ä¿æŒï¼‰
        zip_buffer = create_zip(tmp_dir)

        return {
            'zip': zip_buffer,
            'stats': stats,
            'logs': logs,
            'filename': Path(uploaded_file.name).stem
        }


# ===== UI =====
st.title("ğŸ“¦ å•†å“ç”»åƒä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
st.caption("Excelãƒªã‚¹ãƒˆã«åŸºã¥ãã€ãƒãƒƒãƒˆä¸Šã®å•†å“ç”»åƒã‚’è‡ªå‹•åé›†ãƒ»æ•´ç†ã—ã¾ã™")

with st.container(border=True):
    uploaded_file = st.file_uploader("Excelãƒ•ã‚¡ã‚¤ãƒ«", type=["xlsx"])
    mode = st.radio("å–å¾—ãƒ¢ãƒ¼ãƒ‰", ["å…¨ç”»åƒã‚’å–å¾—", "ãƒ¡ã‚¤ãƒ³ã®ã¿"], horizontal=True)
    st.info(
        "**ä¿å­˜ä»•æ§˜**: 1\_éšå±¤ç®¡ç†ï¼ˆã‚¸ãƒ£ãƒ³ãƒ«åˆ¥ï¼‰ï¼ 2\_ä¸€æ‹¬ç®¡ç†ï¼ˆå…¨é›†ç´„ï¼‰  \n"
        "**å‘½åè¦å‰‡**: æ£šç•ª-æ‹ ç‚¹ã‚³ãƒ¼ãƒ‰-é€£ç•ª.jpg  \n"
        "**ãƒ•ã‚£ãƒ«ã‚¿**: 2KBä»¥ä¸‹ã®ç”»åƒã¯è‡ªå‹•ã‚¹ã‚­ãƒƒãƒ—"
    )

if uploaded_file:
    if st.button("â–¶ å®Ÿè¡Œé–‹å§‹", type="primary", use_container_width=True):
        result = process(uploaded_file, mode == "ãƒ¡ã‚¤ãƒ³ã®ã¿")

        if result:
            st.divider()
            col1, col2, col3 = st.columns(3)
            col1.metric("åˆè¨ˆ", result['stats']['total'])
            col2.metric("æˆåŠŸ", result['stats']['success'])
            col3.metric("æœªå–å¾—", result['stats']['not_found'])

            st.download_button(
                label="ğŸ“¥ çµæœã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆZIPï¼‰",
                data=result['zip'],
                file_name=f"{result['filename']}_images.zip",
                mime="application/zip",
                type="primary",
                use_container_width=True,
            )
